<?xml version="1.0"?>
<doc>
    <assembly>
        <name>SharpAI</name>
    </assembly>
    <members>
        <member name="T:SharpAI.AIDriver">
            <summary>
            AI driver.
            </summary>
        </member>
        <member name="P:SharpAI.AIDriver.Chat">
            <summary>
            Chat APIs.
            </summary>
        </member>
        <member name="P:SharpAI.AIDriver.Completion">
            <summary>
            Commpletion APIs.
            </summary>
        </member>
        <member name="P:SharpAI.AIDriver.Embeddings">
            <summary>
            Embeddings APIs.
            </summary>
        </member>
        <member name="P:SharpAI.AIDriver.Models">
            <summary>
            Model APIs.
            </summary>
        </member>
        <member name="P:SharpAI.AIDriver.Vision">
            <summary>
            Vision APIs.
            </summary>
        </member>
        <member name="P:SharpAI.AIDriver.VisionProjectorPath">
            <summary>
            Gets the configured projector path (mmproj GGUF) or empty string if not configured.
            </summary>
        </member>
        <member name="M:SharpAI.AIDriver.#ctor(SyslogLogging.LoggingModule,System.String,System.String,System.String,System.String)">
            <summary>
            AI driver.
            </summary>
            <param name="logging">Logging module.</param>
            <param name="databaseFilename">Database filename.</param>
            <param name="huggingFaceApiKey">HuggingFace API key.</param>
            <param name="modelDirectory">Model storage directory.</param>
            <param name="multiModalProjectorPath">Path to a LLaVA projector GGUF.</param>
        </member>
        <member name="T:SharpAI.ChatDriver">
            <summary>
            Chat driver for handling chat completions with context optimization.
            </summary>
        </member>
        <member name="P:SharpAI.ChatDriver.DefaultContextTokens">
            <summary>
            Default context tokens.  Default is 4000.  Minimum value is 256.
            </summary>
        </member>
        <member name="P:SharpAI.ChatDriver.ContextUtilizationRatio">
            <summary>
            Context utilization ratio.  Default is 0.3.  Value must be greater than 0 and less than or equal to 1.
            </summary>
        </member>
        <member name="P:SharpAI.ChatDriver.ChunkSize">
            <summary>
            Chunk size when working with context.  Default is 512.  Minimum value is 256.
            </summary>
        </member>
        <member name="P:SharpAI.ChatDriver.ChunkOverlap">
            <summary>
            The amount of overlap that should exist between tokens.  Default is 100.  Minimum value is 0.
            </summary>
        </member>
        <member name="M:SharpAI.ChatDriver.#ctor(SyslogLogging.LoggingModule,SharpAI.Serialization.Serializer,SharpAI.ModelDriver)">
            <summary>
            Chat driver.
            </summary>
            <param name="logging">Logging module.</param>
            <param name="serializer">Serializer.</param>
            <param name="models">Model driver.</param>
        </member>
        <member name="M:SharpAI.ChatDriver.GenerateCompletionStreaming(System.String,System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate a chat completion using streaming.
            Supports context extraction and optimization from formatted prompt strings.
            </summary>
            <param name="model">Model.</param>
            <param name="prompt">Formatted prompt string. Context can be provided in system messages with <context></context> tags.</param>
            <param name="maxTokens">Maximum number of tokens to generate.  Minimum is 100.</param>
            <param name="temperature">Temperature.  Minimum is 0.0, maximum is 1.0</param>
            <param name="token">Cancellation token.</param>
            <returns>Enumerable of strings.</returns>
        </member>
        <member name="M:SharpAI.ChatDriver.GenerateCompletion(System.String,System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate a chat completion without streaming.
            Supports context extraction and optimization from formatted prompt strings.
            </summary>
            <param name="model">Model.</param>
            <param name="prompt">Formatted prompt string. Context can be provided in system messages with <context></context> tags.</param>
            <param name="maxTokens">Maximum number of tokens to generate.  Minimum is 100.</param>
            <param name="temperature">Temperature.  Minimum is 0.0, maximum is 1.0</param>
            <param name="token">Cancellation token.</param>
            <returns>String.</returns>
        </member>
        <member name="T:SharpAI.CompletionDriver">
            <summary>
            Completion driver.
            </summary>
        </member>
        <member name="M:SharpAI.CompletionDriver.#ctor(SyslogLogging.LoggingModule,SharpAI.Serialization.Serializer,SharpAI.ModelDriver)">
            <summary>
            Completion driver.
            </summary>
            <param name="logging">Logging module.</param>
            <param name="serializer">Serializer.</param>
            <param name="models">Model driver.</param>
        </member>
        <member name="M:SharpAI.CompletionDriver.GenerateCompletionStreaming(System.String,System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate a chat completion using streaming.
            </summary>
            <param name="model">Model.</param>
            <param name="prompt">Prompt.</param>
            <param name="maxTokens">Maximum number of tokens to generate.  Minimum is 100.</param>
            <param name="temperature">Temperature.  Minimum is 0.0, maximum is 1.0</param>
            <param name="token">Cancellation token.</param>
            <returns>Enumerable of strings.</returns>
        </member>
        <member name="M:SharpAI.CompletionDriver.GenerateCompletion(System.String,System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate a chat completion without streaming.
            </summary>
            <param name="model">Model.</param>
            <param name="prompt">Prompt.</param>
            <param name="maxTokens">Maximum number of tokens to generate.  Minimum is 100.</param>
            <param name="temperature">Temperature.  Minimum is 0.0, maximum is 1.0</param>
            <param name="token">Cancellation token.</param>
            <returns>String.</returns>
        </member>
        <member name="T:SharpAI.Constants">
            <summary>
            Constants.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.Logo">
            <summary>
            Logo.
            Gracias, as always, to: https://patorjk.com/ using font Rectangles
            </summary>
        </member>
        <member name="F:SharpAI.Constants.HtmlHomepage">
            <summary>
            Default HTML homepage.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.TimestampFormat">
            <summary>
            Timestamp format.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.SettingsFile">
            <summary>
            Settings file.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.DatabaseFile">
            <summary>
            Database filename.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.LogFilename">
            <summary>
            Log filename.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.LogDirectory">
            <summary>
            Log directory.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.ProductName">
            <summary>
            Product name.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.Copyright">
            <summary>
            Copyright.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.BinaryContentType">
            <summary>
            Binary content type.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.JsonContentType">
            <summary>
            JSON content type.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.NdJsonContentType">
            <summary>
            Newline-delimited JSON content type.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.EventStreamContentType">
            <summary>
            Event-stream content type for server-sent events (SSE).
            </summary>
        </member>
        <member name="F:SharpAI.Constants.HtmlContentType">
            <summary>
            HTML content type.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.PngContentType">
            <summary>
            PNG content type.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.TextContentType">
            <summary>
            Text content type.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.FaviconFilename">
            <summary>
            Favicon filename.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.FaviconContentType">
            <summary>
            Favicon content type.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.DefaultHuggingFaceApiKey">
            <summary>
            HuggingFace API key.
            </summary>
        </member>
        <member name="F:SharpAI.Constants.RequestIdHeader">
            <summary>
            SharpAI request ID header.
            </summary>
        </member>
        <member name="T:SharpAI.EmbeddingsDriver">
            <summary>
            Embeddings driver.
            </summary>
        </member>
        <member name="M:SharpAI.EmbeddingsDriver.#ctor(SyslogLogging.LoggingModule,SharpAI.Serialization.Serializer,SharpAI.ModelDriver)">
            <summary>
            Embeddings driver.
            </summary>
            <param name="logging">Logging module.</param>
            <param name="serializer">Serializer.</param>
            <param name="models">Model driver.</param>
        </member>
        <member name="M:SharpAI.EmbeddingsDriver.Generate(System.String,System.String,System.Threading.CancellationToken)">
            <summary>
            Generate embeddings.
            </summary>
            <param name="model">Model.</param>
            <param name="input">String input.</param>
            <param name="token">Cancellation token.</param>
            <returns>Vectors.</returns>
        </member>
        <member name="M:SharpAI.EmbeddingsDriver.Generate(System.String,System.String[],System.Threading.CancellationToken)">
            <summary>
            Generate embeddings.
            </summary>
            <param name="model">Model.</param>
            <param name="inputs">Array of string inputs.</param>
            <param name="token">Cancellation token.</param>
            <returns>Array of vectors.</returns>
        </member>
        <member name="T:SharpAI.Engines.EngineBase">
            <summary>
            Base class for AI providers that support embeddings and text generation.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.EngineBase.IsDisposed">
            <summary>
            Boolean indicating if the engine is disposed.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.EngineBase.ModelPath">
            <summary>
            Path to the model file (ONNX, GGUF, etc.).
            </summary>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GetOptimalGpuLayers">
            <summary>
            Gets the number of GPU layers to use for inference.
            </summary>
            <returns>
            0 = CPU only (no GPU layers)
            -1 = Use all available GPU layers  
            >0 = Use specified number of GPU layers
            </returns>
        </member>
        <member name="P:SharpAI.Engines.EngineBase.EmbeddingDimensions">
            <summary>
            Gets the dimensionality of embeddings produced by this model.
            </summary>
            <returns>Number of dimensions in each embedding vector.</returns>
        </member>
        <member name="P:SharpAI.Engines.EngineBase.SupportsGpu">
            <summary>
            Gets whether this provider supports GPU acceleration.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.EngineBase.SupportsEmbeddings">
            <summary>
            Gets whether this provider supports embeddings.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.EngineBase.SupportsGeneration">
            <summary>
            Gets whether this provider supports chat/text generation.
            </summary>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GetContextSize">
            <summary>
            Gets the context window size (maximum number of tokens) for this model.
            This is used to determine optimal token limits for prompts.
            </summary>
            <returns>The context window size in tokens, or -1 if not available.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.CountTokens(System.String,System.Boolean,System.Boolean)">
            <summary>
            Count tokens in <paramref name="text"/> using the provider's tokenizer.
            </summary>
            <param name="text">Input text to tokenize.</param>
            <param name="addBos">Whether to include a beginning-of-sentence token.</param>
            <param name="parseSpecial">Whether to parse special tokens (e.g., &lt;|im_start|&gt;).</param>
            <returns>Total token count; 0 if the engine is not initialized.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GetDimensionality(System.Threading.CancellationToken)">
            <summary>
            Load the model and determine its dimensionality.
            </summary>
            <param name="token">Cancellation token.</param>
            <returns>Dimensionality.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GenerateEmbeddingsAsync(System.String,System.Threading.CancellationToken)">
            <summary>
            Generates embeddings for a single text input.
            </summary>
            <param name="text">Input text to embed.</param>
            <param name="token">Cancellation token.</param>
            <returns>Embedding vector as a float array.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GenerateEmbeddingsAsync(System.String[],System.Threading.CancellationToken)">
            <summary>
            Generates embeddings for multiple text inputs.
            </summary>
            <param name="texts">Array of input texts to embed.</param>
            <param name="token">Cancellation token.</param>
            <returns>Array of embedding vectors, one per input text.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GenerateTextAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <summary>
            Generates a text completion for the given prompt.
            </summary>
            <param name="prompt">Input prompt.  This prompt should be formatted in a manner appropriate for the model you are using.  For example, "User: {question}\nAssistant: ".</param>
            <param name="maxTokens">Maximum number of tokens to generate.</param>
            <param name="temperature">Sampling temperature (0.0 to 2.0).</param>
            <param name="stopSequences">Sequences that will stop generation.</param>
            <param name="token">Cancellation token.</param>
            <returns>Generated text.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GenerateTextStreamAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <summary>
            Generates a streaming text completion for the given prompt.
            </summary>
            <param name="prompt">Input prompt.  This prompt should be formatted in a manner appropriate for the model you are using.  For example, "User: {question}\nAssistant: ".</param>
            <param name="maxTokens">Maximum number of tokens to generate.</param>
            <param name="temperature">Sampling temperature (0.0 to 2.0).</param>
            <param name="stopSequences">Sequences that will stop generation.</param>
            <param name="token">Cancellation token.</param>
            <returns>Async enumerable of generated text tokens.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GenerateChatCompletionAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <summary>
            Generates a chat completion.
            </summary>
            <param name="prompt">Prompt containing conversation history.  Be sure to format this in accordance with the expectations of the model you are using.</param>
            <param name="maxTokens">Maximum number of tokens to generate.</param>
            <param name="temperature">Sampling temperature (0.0 to 2.0).</param>
            <param name="stopSequences">Sequences that will stop generation.</param>
            <param name="token">Cancellation token.</param>
            <returns>Generated chat response.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.GenerateChatCompletionStreamAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <summary>
            Generates a streaming chat completion given a conversation history.
            </summary>
            <param name="prompt">Prompt containing conversation history.  Be sure to format this in accordance with the expectations of the model you are using.</param>
            <param name="maxTokens">Maximum number of tokens to generate.</param>
            <param name="temperature">Sampling temperature (0.0 to 2.0).</param>
            <param name="stopSequences">Sequences that will stop generation.</param>
            <param name="token">Cancellation token.</param>
            <returns>Async enumerable of generated chat response tokens.</returns>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.InitializeAsync(System.String)">
            <summary>
            Initializes the provider with the specified model.
            </summary>
            <param name="modelPath">Path to the model file.</param>
            <returns>Task representing the initialization.</returns>
        </member>
        <member name="P:SharpAI.Engines.EngineBase.IsInitialized">
            <summary>
            Gets whether the provider is initialized and ready for use.
            </summary>
        </member>
        <member name="M:SharpAI.Engines.EngineBase.Dispose">
            <summary>
            Disposes of the provider and releases resources.
            </summary>
        </member>
        <member name="T:SharpAI.Engines.LlamaSharpEngine">
            <summary>
            LlamaSharp implementation of the AI provider base class.
            Provides text generation, embeddings, chat completion and vision capabilities using the LlamaSharp library.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.LlamaSharpEngine.IsDisposed">
            <summary>
            Boolean indicating if the engine is disposed.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.LlamaSharpEngine.EmbeddingDimensions">
            <summary>
            Gets the number of dimensions in the embedding vectors generated by this engine.
            Returns -1 if not yet determined, 0 if embeddings are not supported.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.LlamaSharpEngine.SupportsGpu">
            <summary>
            Gets a value indicating whether this engine supports GPU acceleration.
            LlamaSharp supports GPU acceleration when CUDA is available.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.LlamaSharpEngine.SupportsEmbeddings">
            <summary>
            Gets a value indicating whether this engine supports generating embeddings.
            Support depends on whether the embedder was successfully initialized.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.LlamaSharpEngine.SupportsGeneration">
            <summary>
            Gets a value indicating whether this engine supports text generation.
            LlamaSharp always supports text generation.
            </summary>
        </member>
        <member name="P:SharpAI.Engines.LlamaSharpEngine.IsInitialized">
            <summary>
            Gets whether this engine has been successfully initialized.
            </summary>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GetContextSize">
            <summary>
            Gets the context window size (maximum number of tokens) for this model.
            </summary>
            <returns>The context window size in tokens, or -1 if not available.</returns>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.CountTokens(System.String,System.Boolean,System.Boolean)">
            <summary>
            Count tokens in <paramref name="text"/> using the provider's tokenizer.
            </summary>
            <param name="text">Input text to tokenize.</param>
            <param name="addBos">Whether to include a beginning-of-sentence token.</param>
            <param name="parseSpecial">Whether to parse special tokens (e.g., &lt;|im_start|&gt;).</param>
            <returns>Total token count; 0 if the engine is not initialized.</returns>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.#ctor(SyslogLogging.LoggingModule)">
            <summary>
            Initializes a new instance of the LlamaSharpEngine class.
            </summary>
            <param name="logging">Optional logging module for capturing debug and error information. If null, a new LoggingModule will be created.</param>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.Dispose">
            <summary>
            Releases all resources used by the LlamaSharpEngine.
            Properly disposes of the model, context, and embedder instances.
            </summary>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.InitializeAsync(System.String)">
            <summary>
            Initializes the LlamaSharp engine with the specified model file.
            Loads the model, creates context, and sets up executors for text generation and embeddings.
            </summary>
            <param name="modelPath">The file path to the GGUF model file to load.</param>
            <returns>A task that represents the asynchronous initialization operation.</returns>
            <exception cref="T:System.Exception">Thrown when the model fails to load or initialize.</exception>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GetOptimalGpuLayers">
            <summary>
            Determines the optimal number of GPU layers to use based on available hardware.
            </summary>
            <returns>The number of GPU layers to use, or 0 if GPU acceleration is not available. Returns -1 to use all available GPU layers.</returns>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GetDimensionality(System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateEmbeddingsAsync(System.String,System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateEmbeddingsAsync(System.String[],System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateTextAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateTextStreamAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateChatCompletionAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateChatCompletionStreamAsync(System.String,System.Int32,System.Single,System.String[],System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.ConfigureVision(System.String)">
            <summary>
            Configure LLaVA projector (mmproj) for vision. If a path is provided it must exist.
            </summary>
            <param name="multiModalProjectorPath">Full path to mmproj GGUF or a directory to search; optional.</param>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateVisionCompletionAsync(System.Collections.Generic.IEnumerable{System.Byte[]},System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate vision completion with image bytes
            </summary>
            <param name="imagesBytes">Collection of image byte arrays</param>
            <param name="prompt">Text prompt to combine with the images</param>
            <param name="maxTokens">Maximum number of tokens to generate</param>
            <param name="temperature">Sampling temperature (0.0 to 1.0)</param>
            <param name="token">Cancellation token</param>
            <returns>Generated text response</returns>
        </member>
        <member name="M:SharpAI.Engines.LlamaSharpEngine.GenerateVisionCompletionStreamAsync(System.Collections.Generic.IEnumerable{System.Byte[]},System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate streaming vision completion with image bytes.
            Provides real-time token streaming for vision model responses.
            </summary>
            <param name="imagesBytes">Collection of image byte arrays</param>
            <param name="prompt">Text prompt to combine with the images</param>
            <param name="maxTokens">Maximum number of tokens to generate</param>
            <param name="temperature">Sampling temperature (0.0 to 1.0)</param>
            <param name="token">Cancellation token</param>
            <returns>Async enumerable of text chunks</returns>
        </member>
        <member name="T:SharpAI.Helpers.DataTableHelper">
            <summary>
            Data table helper.
            </summary>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetBooleanValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve Boolean value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Boolean value.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetNullableBooleanValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve nullable Boolean value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName">Name of the column.</param>
            <returns>Boolean value if valid, null if unknown or invalid.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetStringValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve string value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>String.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetInt32Value(System.Data.DataRow,System.String)">
            <summary>
            Retrieve integer value.  Returns 0 if the cell is null.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Integer.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetNullableInt32Value(System.Data.DataRow,System.String)">
            <summary>
            Retrieve nullable integer value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Integer.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetInt64Value(System.Data.DataRow,System.String)">
            <summary>
            Retrieve long value.  Returns 0 if the cell is null.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Long.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetNullableInt64Value(System.Data.DataRow,System.String)">
            <summary>
            Retrieve nullable integer value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Integer.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetGuidValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve GUID value.  Returns the default GUID if the cell is null.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>GUID.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetNullableGuidValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve nullable GUID value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>GUID.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetEnumValue``1(System.Data.DataRow,System.String)">
            <summary>
            Retrieve enum value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Enum.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetDateTimeValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve DateTime value.  Returns the default DateTime if the cell is null.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>DateTime.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetNullableDateTimeValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve nullable DateTime value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Nullable DateTime.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetDecimalValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve decimal value.  Returns 0 if the cell is null.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Decimal.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetNullableDecimalValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve nullable decimal value.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Nullable decimal.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetNullableBinaryValue(System.Data.DataRow,System.String)">
            <summary>
            Retrieve binary value or null.
            </summary>
            <param name="row">DataRow.</param>
            <param name="columnName"></param>
            <returns>Byte array or null.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.DataTableToListDynamic(System.Data.DataTable)">
            <summary>
            Convert a data table to a dynamic list.
            </summary>
            <param name="dt">Data table.</param>
            <returns>List of dynamic.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.DataTableToDynamic(System.Data.DataTable)">
            <summary>
            Convert a data table to a dynamic.
            </summary>
            <param name="dt">Data table.</param>
            <returns>Dynamic.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.DataTableToListDictionary(System.Data.DataTable)">
            <summary>
            Convert a data table to a list of dictionaries.
            </summary>
            <param name="dt">Data table.</param>
            <returns>List of dictionaries.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.DataTableToDictionary(System.Data.DataTable)">
            <summary>
            Convert a data table to a dictionary.
            </summary>
            <param name="dt">Data table.</param>
            <returns>Dictionary.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.ObjectToDictionary(System.Object)">
            <summary>
            Convert an object to a dictionary.
            </summary>
            <param name="obj">Object.</param>
            <returns>Dictionary.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.IsDictionary(System.Object)">
            <summary>
            Check if an object is a dictionary.
            </summary>
            <param name="obj">Object.</param>
            <returns>True if dictionary.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.DataTableToListGuid(System.Data.DataTable,System.String)">
            <summary>
            Extract a list of GUIDs from a table by column name.
            </summary>
            <param name="table">Data table.</param>
            <param name="columnName">Column name.</param>
            <returns>List of GUIDs.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DataTableHelper.GetLength(System.Data.DataTable)">
            <summary>
            Calculate DataTable length.
            This calculation adds the length of column names and row values.
            </summary>
            <param name="dt">DataTable.</param>
            <returns>Length.</returns>
        </member>
        <member name="T:SharpAI.Helpers.DirectoryHelper">
            <summary>
            Directory helper.
            </summary>
        </member>
        <member name="M:SharpAI.Helpers.DirectoryHelper.NormalizeDirectory(System.String)">
            <summary>
            Normalize directory path.
            </summary>
            <param name="directory">Directory.</param>
            <returns>Normalized directory.</returns>
        </member>
        <member name="M:SharpAI.Helpers.DirectoryHelper.RecursiveDelete(System.String,System.Boolean)">
            <summary>
            Recursively delete a directory.
            </summary>
            <param name="baseDir">Base directory.</param>
            <param name="deleteRootDirectory">True to delete the root directory.</param>
        </member>
        <member name="M:SharpAI.Helpers.DirectoryHelper.RecursiveDelete(System.IO.DirectoryInfo,System.Boolean)">
            <summary>
            Recursively delete a directory.
            </summary>
            <param name="baseDir">Base directory.</param>
            <param name="deleteRootDirectory">True to delete the root directory.</param>
        </member>
        <member name="T:SharpAI.Helpers.GgufSelector">
            <summary>
            GGUF selector.
            </summary>
        </member>
        <member name="M:SharpAI.Helpers.GgufSelector.SortByOllamaPreference(System.Collections.Generic.IEnumerable{SharpAI.Hosting.HuggingFace.GgufFileInfo})">
            <summary>
            Sorts a list of GGUF files based on Ollama's quantization preference order.
            </summary>
            <param name="files">The list of GGUF files to sort.</param>
            <returns>A sorted list of GGUF files ordered by Ollama's preference.</returns>
        </member>
        <member name="M:SharpAI.Helpers.GgufSelector.SortByPreference(System.Collections.Generic.IEnumerable{SharpAI.Hosting.HuggingFace.GgufFileInfo},System.Collections.Generic.Dictionary{System.String,System.Int32})">
            <summary>
            Sorts a list of GGUF files based on user-supplied quantization priority.
            </summary>
            <param name="files">The list of GGUF files to sort.</param>
            <param name="quantizationPriority">User-supplied quantization priority.</param>
            <returns>A sorted list of GGUF files ordered by Ollama's preference.</returns>
        </member>
        <member name="M:SharpAI.Helpers.GgufSelector.ExtractBaseQuantizationType(System.String)">
            <summary>
            Extracts the base quantization type from a potentially complex quantization string.
            </summary>
        </member>
        <member name="T:SharpAI.Helpers.HashHelper">
            <summary>
            Hash methods.
            </summary>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.MD5Hash(System.Byte[])">
            <summary>
            Generate an MD5 hash.
            </summary>
            <param name="data">Data.</param>
            <returns>MD5 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.MD5Hash(System.IO.Stream)">
            <summary>
            Generate an MD5 hash.
            </summary>
            <param name="stream">Stream.</param>
            <returns>MD5 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.MD5Hash(System.String)">
            <summary>
            Generate an MD5 hash of a string.
            </summary>
            <param name="str">String.</param>
            <returns>MD5 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.MD5Hash(System.Collections.Generic.List{System.String})">
            <summary>
            Generate an MD5 hash of a list of strings.
            </summary>
            <param name="strings">Strings.</param>
            <returns>MD5 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.MD5Hash(System.Data.DataTable)">
            <summary>
            Generate an MD5 hash of a DataTable.
            This method concatenates column names (separated by a null character) and all cell values (separated by a null character).  Any null cells have their value replaced with the string NULL.
            </summary>
            <param name="dt">DataTable. </param>
            <returns>MD5 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA1Hash(System.Byte[])">
            <summary>
            Generate a SHA1 hash of a byte array.
            </summary>
            <param name="data">Data.</param>
            <returns>SHA1 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA1Hash(System.IO.Stream)">
            <summary>
            Generate a SHA1 hash.
            </summary>
            <param name="stream">Stream.</param>
            <returns>MD5 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA1Hash(System.String)">
            <summary>
            Generate a SHA1 hash of a string.
            </summary>
            <param name="str">String.</param>
            <returns>SHA1 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA1Hash(System.Collections.Generic.List{System.String})">
            <summary>
            Generate a SHA1 hash of a list of strings.
            </summary>
            <param name="strings">Strings.</param>
            <returns>SHA1 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA1Hash(System.Data.DataTable)">
            <summary>
            Generate a SHA1 hash of a DataTable.
            This method concatenates column names (separated by a null character) and all cell values (separated by a null character).  Any null cells have their value replaced with the string NULL.
            </summary>
            <param name="dt">DataTable. </param>
            <returns>SHA1 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA256Hash(System.Byte[])">
            <summary>
            Generate a SHA256 hash of a byte array.
            </summary>
            <param name="data">Data.</param>
            <returns>SHA256 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA256Hash(System.IO.Stream)">
            <summary>
            Generate a SHA256 hash.
            </summary>
            <param name="stream">Stream.</param>
            <returns>MD5 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA256Hash(System.String)">
            <summary>
            Generate a SHA256 hash of a string.
            </summary>
            <param name="str">String.</param>
            <returns>SHA256 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA256Hash(System.Collections.Generic.List{System.String})">
            <summary>
            Generate a SHA256 hash of a list of strings.
            </summary>
            <param name="strings">Strings.</param>
            <returns>SHA256 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.SHA256Hash(System.Data.DataTable)">
            <summary>
            Generate a SHA256 hash of a DataTable.
            This method concatenates column names (separated by a null character) and all cell values (separated by a null character).  Any null cells have their value replaced with the string NULL.
            </summary>
            <param name="dt">DataTable. </param>
            <returns>SHA256 hash.</returns>
        </member>
        <member name="M:SharpAI.Helpers.HashHelper.ComputeAllHashes(System.IO.Stream)">
            <summary>
            Compute MD5, SHA1, and SHA256 hashes in a single pass.
            </summary>
            <param name="stream">Stream.</param>
            <returns>Tuple containing MD5, SHA1, and SHA256 hashes.</returns>
        </member>
        <member name="T:SharpAI.Helpers.StreamHelper">
            <summary>
            Stream helpers.
            </summary>
        </member>
        <member name="M:SharpAI.Helpers.StreamHelper.ReadFully(System.IO.Stream,System.Int32)">
            <summary>
            Read a stream fully.
            </summary>
            <param name="stream">Stream.</param>
            <param name="position">Position from which to read.</param>
            <returns>Bytes.</returns>
        </member>
        <member name="T:SharpAI.Helpers.StringHelpers">
            <summary>
            String collection helpers.
            </summary>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.Combine(System.Collections.Generic.List{System.String},System.Collections.Generic.List{System.String})">
            <summary>
            Combine two string lists.
            </summary>
            <param name="list1">String list 1.</param>
            <param name="list2">String list 2.</param>
            <returns>String list.</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.RedactTail(System.String,System.String,System.Int32)">
            <summary>
            Redact a string.
            </summary>
            <param name="str">String.</param>
            <param name="replacementChar">Replacement character.</param>
            <param name="charsToRetain">Number of characters to retain.</param>
            <returns>Redacted string.</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.StringToGuidList(System.String)">
            <summary>
            Convert a CSV string list of GUIDs to a List of Guid.
            </summary>
            <param name="str">Input string.</param>
            <returns>List of GUIDs.</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.IsValidBase64(System.String)">
            <summary>
            Tests if a string is a valid base64 encoded string.
            </summary>
            <param name="input">String.</param>
            <returns>True if valid.</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.IsValidHex(System.String)">
            <summary>
            Tests if a string is a valid hexadecimal string.
            </summary>
            <param name="input">String.</param>
            <returns>True if valid.</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.IsValidHex(System.String,System.Boolean)">
            <summary>
            Tests if a string is a valid hexadecimal string with strict length requirement.
            </summary>
            <param name="input">String.</param>
            <param name="requireEvenLength">If true, requires even number of hex digits (for byte representation).</param>
            <returns>True if valid.</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.IsValidEmail(System.String)">
            <summary>
            Tests if a string is a reasonable email address format
            </summary>
            <param name="input">The string to test</param>
            <returns>True if the string appears to be a valid email format, false otherwise</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.IsValidLocalPart(System.String)">
            <summary>
            Validates the local part (before @) of an email address
            </summary>
            <param name="localPart">The local part to validate</param>
            <returns>True if valid, false otherwise</returns>
        </member>
        <member name="M:SharpAI.Helpers.StringHelpers.IsValidDomainPart(System.String)">
            <summary>
            Validates the domain part (after @) of an email address
            </summary>
            <param name="domainPart">The domain part to validate</param>
            <returns>True if valid, false otherwise</returns>
        </member>
        <member name="T:SharpAI.Helpers.TimeHelper">
            <summary>
            Time helper.
            </summary>
        </member>
        <member name="M:SharpAI.Helpers.TimeHelper.TotalMsBetween(System.DateTime,System.DateTime,System.Int32)">
            <summary>
            Determine the total number of milliseconds between a start and end time.
            </summary>
            <param name="start">Start time.</param>
            <param name="end">End time.</param>
            <param name="decimalPlaces">Number of decimal places.</param>
            <returns>Milliseconds.</returns>
        </member>
        <member name="T:SharpAI.Hosting.CardData">
            <summary>
            Represents the model card metadata including license, language, and datasets.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.Language">
            <summary>
            Gets or sets the language of the model (e.g., "en" for English) or list of languages.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.License">
            <summary>
            Gets or sets the license under which the model is distributed (e.g., "apache-2.0").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.LibraryName">
            <summary>
            Gets or sets the library name from the model card.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.Tags">
            <summary>
            Gets or sets the list of tags from the model card.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.Datasets">
            <summary>
            Gets or sets the list of datasets used to train the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.PipelineTag">
            <summary>
            Gets or sets the pipeline tag from the model card.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.BaseModel">
            <summary>
            Gets or sets the base model that this model was derived from.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.ModelCreator">
            <summary>
            Gets or sets the name of the person or organization that created the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.ModelName">
            <summary>
            Gets or sets the human-readable name of the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.QuantizedBy">
            <summary>
            Gets or sets the name of the person or organization that quantized the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.Inference">
            <summary>
            Gets or sets whether inference is enabled for this model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.ModelType">
            <summary>
            Gets or sets the model type from the card data.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.TaskCategories">
            <summary>
            Gets or sets the task categories for the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.CardData.TaskIds">
            <summary>
            Gets or sets the task IDs for the model.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.GatedRequest">
            <summary>
            Represents gated access request configuration for models with access control.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GatedRequest.Gate">
            <summary>
            Gets or sets the gate type (e.g., "manual", "automatic").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GatedRequest.GatedPrompt">
            <summary>
            Gets or sets the custom prompt shown to users requesting access.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GatedRequest.ExtraFields">
            <summary>
            Gets or sets additional fields required for access requests.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.HuggingFace.GgufFileInfo">
            <summary>
            Represents a GGUF file with enhanced metadata including quantization information.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFace.GgufFileInfo.SizeFormatted">
            <summary>
            Gets or sets the human-readable formatted file size.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFace.GgufFileInfo.QuantizationType">
            <summary>
            Gets or sets the detected quantization type (e.g., "Q4_K_M", "Q5_K_S").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFace.GgufFileInfo.IsMainModel">
            <summary>
            Gets or sets a value indicating whether this is the main model file (not a shard).
            </summary>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFace.GgufFileInfo.#ctor">
            <summary>
            Initializes a new instance of the GgufFileInfo class.
            </summary>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFace.GgufFileInfo.ToString">
            <summary>
            Returns a string representation of the GGUF file information.
            </summary>
            <returns>A formatted string containing GGUF-specific information.</returns>
        </member>
        <member name="T:SharpAI.Hosting.GgufInfo">
            <summary>
            GGUF info.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GgufInfo.Total">
            <summary>
            Gets or sets the total size of the model in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GgufInfo.Architecture">
            <summary>
            Gets or sets the model architecture (e.g., "bert", "llama", "gpt2").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GgufInfo.ContextLength">
            <summary>
            Gets or sets the maximum context length in tokens that the model can process.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GgufInfo.Causal">
            <summary>
            Gets or sets a value indicating whether the model is causal (autoregressive) or not.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GgufInfo.BosToken">
            <summary>
            Gets or sets the beginning-of-sequence token (e.g., "[CLS]" for BERT).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.GgufInfo.EosToken">
            <summary>
            Gets or sets the end-of-sequence token (e.g., "[SEP]" for BERT).
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.HuggingFaceClient">
            <summary>
            Client for interacting with HuggingFace model repositories to retrieve file information and download files.
            </summary>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.#ctor(SyslogLogging.LoggingModule,System.String)">
            <summary>
            Initializes a new instance of the HuggingFace client.
            </summary>
            <param name="logging">LoggingModule instance for logging operations.</param>
            <param name="apiKey">HuggingFace API key for authentication.</param>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.GetModelMetadata(System.String,System.Threading.CancellationToken)">
            <summary>
            Retrieve model metadata.
            </summary>
            <param name="modelName">Model name.</param>
            <param name="token">Cancellation token.</param>
            <returns>HuggingFaceModelMetadata.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.GetModelFilesAsync(System.String,System.Threading.CancellationToken)">
            <summary>
            Retrieves all files from a HuggingFace model repository.
            </summary>
            <param name="modelName">The name of the model (e.g., "microsoft/DialoGPT-medium").</param>
            <param name="token">Cancellation token.</param>
            <returns>A list of HuggingFaceModelFile objects representing all files in the repository.</returns>
            <exception cref="T:System.Exception">Thrown when API request fails or JSON parsing fails.</exception>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.GetGgufFilesAsync(System.String,System.Threading.CancellationToken)">
            <summary>
            Retrieves only GGUF files from a HuggingFace model repository.
            </summary>
            <param name="modelName">The name of the model to search for GGUF files.</param>
            <param name="token">Cancellation token.</param>
            <returns>A list of GgufFileInfo objects containing only .gguf files with enhanced metadata.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.GetGgufFilesByQuantizationAsync(System.String,System.String,System.Threading.CancellationToken)">
            <summary>
            Retrieves GGUF files filtered by a specific quantization type.
            </summary>
            <param name="modelName">The name of the model to search.</param>
            <param name="quantizationType">The quantization type to filter by (e.g., "Q4_K_M", "Q5_K_S").</param>
            <param name="token">Cancellation token.</param>
            <returns>A list of GgufFileInfo objects matching the specified quantization type.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.GetAvailableQuantizationTypesAsync(System.String,System.Threading.CancellationToken)">
            <summary>
            Retrieves all available quantization types for GGUF files in a model repository.
            </summary>
            <param name="modelName">The name of the model to analyze.</param>
            <param name="token">Cancellation token.</param>
            <returns>A sorted list of unique quantization type strings.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.GetDownloadUrls(System.String,SharpAI.Hosting.HuggingFaceModelFile)">
            <summary>
            Gets the proper download URLs for a HuggingFace file, prioritizing the most reliable options.
            </summary>
            <param name="modelName">The model name (e.g., "microsoft/phi-2").</param>
            <param name="file">The HuggingFaceModelFile containing path and OID.</param>
            <returns>List of URLs to try in order of preference.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.TestDownloadUrlAsync(System.String,System.Threading.CancellationToken)">
            <summary>
            Tests if a download URL is accessible.
            </summary>
            <param name="url">The URL to test.</param>
            <param name="token">Cancellation token.</param>
            <returns>True if the URL is accessible, false otherwise.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.TryDownloadFileAsync(System.String,System.String,System.Action{System.String,System.Int64,System.Decimal},System.Threading.CancellationToken)">
            <summary>
            Try to download a file from HuggingFace to the specified destination.
            </summary>
            <param name="sourceUrl">The HuggingFace URL of the file to download.</param>
            <param name="destinationFilename">The full path where the file should be saved.</param>
            <param name="progressCallback">Progress callback.</param>
            <param name="token">Cancellation token.</param>
            <returns>True if successful.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.DownloadFileAsync(System.String,System.String,System.Action{System.String,System.Int64,System.Decimal},System.Threading.CancellationToken)">
            <summary>
            Downloads a file from HuggingFace to the specified destination.
            </summary>
            <param name="sourceUrl">The HuggingFace URL of the file to download.</param>
            <param name="destinationFilename">The full path where the file should be saved.</param>
            <param name="progressCallback">Progress callback (URL, bytes downloaded, percentage 0-1).</param>
            <param name="token">Cancellation token.</param>
            <returns>Task representing the asynchronous download operation.</returns>
            <exception cref="T:System.ArgumentNullException">Thrown when destinationFilename or sourceUrl is null or empty.</exception>
            <exception cref="T:System.Exception">Thrown when download fails or file operations fail.</exception>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.DownloadFilesAsync(System.String,System.Collections.Generic.List{SharpAI.Hosting.HuggingFaceModelFile},System.String,System.Action{System.String,System.Int64,System.Decimal},System.Threading.CancellationToken)">
            <summary>
            Downloads multiple files from HuggingFace with progress tracking and error handling.
            </summary>
            <param name="modelName">The name of the model.</param>
            <param name="files">List of files to download.</param>
            <param name="downloadDirectory">Directory to save files to.</param>
            <param name="progressCallback">Progress callback (URL, bytes downloaded, percentage 0-1).</param>
            <param name="token">Cancellation token.</param>
            <returns>Number of successfully downloaded files.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.ExtractQuantizationFromFilename(System.String)">
            <summary>
            Extract quantization from filename.
            </summary>
            <param name="filePath">File path.</param>
            <returns>Quantization.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.ExtractQuantizationFromUrl(System.String)">
            <summary>
            Extract quantization from URL.
            </summary>
            <param name="url">URL.</param>
            <returns>Quantization.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.IsEmbeddingModel(SharpAI.Hosting.HuggingFaceModelMetadata)">
            <summary>
            Test if a model can be used for embeddings.
            </summary>
            <param name="md">Model metadata.</param>
            <returns>True if the model can be used for embeddings.</returns>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceClient.IsCompletionModel(SharpAI.Hosting.HuggingFaceModelMetadata)">
            <summary>
            Test if a model can be used for completions.
            </summary>
            <param name="md">Model metadata.</param>
            <returns>True if the model can be used for completions.</returns>
        </member>
        <member name="T:SharpAI.Hosting.HuggingFaceModelFile">
            <summary>
            Represents a file from a HuggingFace model repository with metadata.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelFile.Path">
            <summary>
            Gets or sets the file path within the repository.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelFile.Type">
            <summary>
            Gets or sets the file type ("file" or "directory").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelFile.Size">
            <summary>
            Gets or sets the file size in bytes (null for directories).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelFile.LastModified">
            <summary>
            Gets or sets the last modified timestamp.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelFile.Oid">
            <summary>
            Gets or sets the Git object ID.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelFile.Lfs">
            <summary>
            Gets or sets the Large File Storage information.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelFile.SecurityStatus">
            <summary>
            Gets or sets the security scan status.
            </summary>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceModelFile.#ctor">
            <summary>
            Initializes a new instance of the HuggingFaceModelFile class.
            </summary>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceModelFile.ToString">
            <summary>
            Returns a string representation of the HuggingFace model file.
            </summary>
            <returns>A formatted string containing path, type, and size information.</returns>
        </member>
        <member name="T:SharpAI.Hosting.HuggingFaceModelMetadata">
            <summary>
            HuggingFace model metadata.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Id">
            <summary>
            Gets or sets the internal database ID of the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.ModelId">
            <summary>
            Gets or sets the full model ID in the format "organization/model-name".
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.ModelIdentifier">
            <summary>
            Gets or sets the model identifier (same as id field).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Author">
            <summary>
            Gets or sets the author or organization that created the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Sha">
            <summary>
            Gets or sets the Git SHA hash of the current model version.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.CreatedAt">
            <summary>
            Gets or sets the timestamp of when the model was created.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.LastModified">
            <summary>
            Gets or sets the timestamp of when the model was last modified.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Private">
            <summary>
            Gets or sets a value indicating whether the model is private.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Gated">
            <summary>
            Gets or sets a value indicating whether the model requires authentication to access.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Disabled">
            <summary>
            Gets or sets a value indicating whether the model is disabled.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.PipelineTag">
            <summary>
            Gets or sets the pipeline tag indicating the model's primary task (e.g., "sentence-similarity", "text-generation").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Tags">
            <summary>
            Gets or sets the list of tags associated with the model, including frameworks, tasks, languages, and datasets.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Downloads">
            <summary>
            Gets or sets the total number of downloads for the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.DownloadsAllTime">
            <summary>
            Gets or sets the total downloads across all time.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Likes">
            <summary>
            Gets or sets the number of likes/stars the model has received.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.TrendingScore">
            <summary>
            Gets or sets the trending score of the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.LibraryName">
            <summary>
            Gets or sets the primary library used for the model (e.g., "sentence-transformers", "transformers").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.MaskToken">
            <summary>
            Gets or sets the mask token used by the model's tokenizer (e.g., "[MASK]" for BERT models).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.WidgetData">
            <summary>
            Gets or sets the widget data used for the model's interactive demo examples.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.ModelIndex">
            <summary>
            Gets or sets the model index information (typically null).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Config">
            <summary>
            Gets or sets the model configuration including architecture and tokenizer settings.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.CardData">
            <summary>
            Gets or sets the model card data containing metadata like license, language, and datasets.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.TransformersInfo">
            <summary>
            Gets or sets the transformers library specific information for loading the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.SafeTensors">
            <summary>
            Gets or sets the SafeTensors format information including parameter counts by data type.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Gguf">
            <summary>
            Gets or sets the GGUF format information for quantized models.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Siblings">
            <summary>
            Gets or sets the list of files (siblings) in the model repository.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Spaces">
            <summary>
            Gets or sets the list of Hugging Face Spaces that use this model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Inference">
            <summary>
            Gets or sets the inference status (e.g., "warm" indicating the model is loaded).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.UsedStorage">
            <summary>
            Gets or sets the total storage used by the model in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.GatedRequest">
            <summary>
            Gets or sets the gated request configuration for models with access control.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.PapersWithCodeId">
            <summary>
            Gets or sets the papers with code ID if the model is linked to a paper.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Description">
            <summary>
            Gets or sets the description of the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.Citation">
            <summary>
            Gets or sets the citation information for the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.ResourceGroup">
            <summary>
            Gets or sets the resource group associated with the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.HuggingFaceModelMetadata.XetEnabled">
            <summary>
            Gets or sets whether XET is enabled for the model.
            </summary>
        </member>
        <member name="M:SharpAI.Hosting.HuggingFaceModelMetadata.#ctor">
            <summary>
            Initializes a new instance of the HuggingFaceModelMetadata class.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.LfsInfo">
            <summary>
            Represents Large File Storage information for a file.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.LfsInfo.Sha256">
            <summary>
            Gets or sets the SHA256 hash of the file.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.LfsInfo.Size">
            <summary>
            Gets or sets the size of the file in LFS.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.LfsInfo.PointerSize">
            <summary>
            Gets or sets the pointer size.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.ModelConfig">
            <summary>
            Represents the model's configuration including architecture and tokenizer settings.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.Architectures">
            <summary>
            Gets or sets the list of model architectures (e.g., ["BertModel"]).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.ModelType">
            <summary>
            Gets or sets the type of model architecture (e.g., "bert", "llama", "gpt2").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.TokenizerConfig">
            <summary>
            Gets or sets the tokenizer configuration with special tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.AdditionalChatTemplates">
            <summary>
            Gets or sets additional chat templates for conversational models.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.MaxPositionEmbeddings">
            <summary>
            Gets or sets the maximum position embeddings (context length).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.HiddenSize">
            <summary>
            Gets or sets the hidden size of the model.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.NumAttentionHeads">
            <summary>
            Gets or sets the number of attention heads.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.NumHiddenLayers">
            <summary>
            Gets or sets the number of hidden layers.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.ModelConfig.VocabSize">
            <summary>
            Gets or sets the vocabulary size.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.SafeTensorParameters">
            <summary>
            Represents the breakdown of model parameters by data type.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensorParameters.I64">
            <summary>
            Gets or sets the number of 64-bit integer parameters (typically for embeddings or indices).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensorParameters.F32">
            <summary>
            Gets or sets the number of 32-bit floating-point parameters (the main model weights).
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensorParameters.F16">
            <summary>
            Gets or sets the number of 16-bit floating-point parameters.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensorParameters.BF16">
            <summary>
            Gets or sets the number of brain float 16-bit parameters.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensorParameters.I8">
            <summary>
            Gets or sets the number of 8-bit integer parameters.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensorParameters.U8">
            <summary>
            Gets or sets the number of unsigned 8-bit integer parameters.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.SafeTensors">
            <summary>
            Represents SafeTensors format information including parameter counts.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensors.Parameters">
            <summary>
            Gets or sets the parameter counts broken down by data type.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.SafeTensors.Total">
            <summary>
            Gets or sets the total number of parameters across all data types.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.Sibling">
            <summary>
            Represents a file in the model repository.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.Sibling.RFilename">
            <summary>
            Gets or sets the relative filename within the repository (e.g., "config.json", "model.safetensors").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.Sibling.Size">
            <summary>
            Gets or sets the file size in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.Sibling.BlobId">
            <summary>
            Gets or sets the blob ID for the file.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.Sibling.Lfs">
            <summary>
            Gets or sets the LFS (Large File Storage) information.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.TokenizerConfig">
            <summary>
            Represents the tokenizer configuration with special tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.UnkToken">
            <summary>
            Gets or sets the unknown token (typically "[UNK]").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.SepToken">
            <summary>
            Gets or sets the separator token (typically "[SEP]").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.PadToken">
            <summary>
            Gets or sets the padding token (typically "[PAD]").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.ClsToken">
            <summary>
            Gets or sets the classification token (typically "[CLS]").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.MaskToken">
            <summary>
            Gets or sets the mask token (typically "[MASK]").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.BosToken">
            <summary>
            Gets or sets the beginning of sequence token.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.EosToken">
            <summary>
            Gets or sets the end of sequence token.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TokenizerConfig.ChatTemplate">
            <summary>
            Gets or sets the chat template for conversational models.
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.TransformersInfo">
            <summary>
            Represents transformers library specific information.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TransformersInfo.AutoModel">
            <summary>
            Gets or sets the auto model class to use for loading (e.g., "AutoModel", "AutoModelForCausalLM").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TransformersInfo.PipelineTag">
            <summary>
            Gets or sets the pipeline tag for transformers inference (e.g., "feature-extraction", "text-generation").
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.TransformersInfo.Processor">
            <summary>
            Gets or sets the processor/tokenizer class to use (e.g., "AutoTokenizer").
            </summary>
        </member>
        <member name="T:SharpAI.Hosting.WidgetData">
            <summary>
            Represents example data for the model's interactive widget.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.WidgetData.SourceSentence">
            <summary>
            Gets or sets the source sentence for comparison in similarity tasks.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.WidgetData.Sentences">
            <summary>
            Gets or sets the list of sentences to compare against the source for similarity scoring.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.WidgetData.Text">
            <summary>
            Gets or sets the text input for generation tasks.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.WidgetData.Context">
            <summary>
            Gets or sets the context for question-answering tasks.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.WidgetData.Question">
            <summary>
            Gets or sets the question for question-answering tasks.
            </summary>
        </member>
        <member name="P:SharpAI.Hosting.WidgetData.ExampleInputs">
            <summary>
            Gets or sets example inputs for the widget.
            </summary>
        </member>
        <member name="T:SharpAI.ModelDriver">
            <summary>
            Model driver.
            </summary>
        </member>
        <member name="P:SharpAI.ModelDriver.HuggingFaceApiKey">
            <summary>
            HuggingFace API key.
            </summary>
        </member>
        <member name="P:SharpAI.ModelDriver.ModelDirectory">
            <summary>
            Model directory.
            </summary>
        </member>
        <member name="M:SharpAI.ModelDriver.#ctor(SyslogLogging.LoggingModule,Watson.ORM.Sqlite.WatsonORM,SharpAI.Serialization.Serializer,System.String,System.String)">
            <summary>
            Model driver.
            </summary>
            <param name="logging">Logging module.</param>
            <param name="orm">ORM.</param>
            <param name="serializer">Serializer.</param>
            <param name="huggingFaceApiKey">HuggingFace API key.</param>
            <param name="modelDirectory">Model storage directory.</param>
        </member>
        <member name="M:SharpAI.ModelDriver.All">
            <summary>
            Retrieve all.
            </summary>
            <returns></returns>
        </member>
        <member name="M:SharpAI.ModelDriver.Enumerate(System.Nullable{System.Guid},System.Int32,System.Int32,System.Collections.Generic.Dictionary{System.String,System.String},SharpAI.Models.EnumerationOrderEnum)">
            <summary>
            Enumerate.
            </summary>
            <param name="continuationToken">Continuation token.</param>
            <param name="maxResults">Maximum number of results to retrieve.</param>
            <param name="skip">The number of records to skip.</param>
            <param name="filter">Filters to add to the request.</param>
            <param name="ordering">Ordering.</param>
            <returns>Enumeration result.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.GetByGuid(System.Guid)">
            <summary>
            Get by GUID.
            </summary>
            <param name="guid">GUID.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.GetByName(System.String)">
            <summary>
            Get by name.
            </summary>
            <param name="name">Name.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.GetFilename(System.String)">
            <summary>
            Retrieve the full path and filename of a given model by name.
            </summary>
            <param name="name">Name.</param>
            <returns>Path and filename.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.GetMany(System.Collections.Generic.List{System.Guid})">
            <summary>
            Get many.
            </summary>
            <param name="guids">GUIDs.</param>
            <returns>List.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.ExistsByGuid(System.Guid)">
            <summary>
            Exists by GUID.
            </summary>        
            <param name="guid">GUID.</param>
            <returns>True if exists.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.First(ExpressionTree.Expr)">
            <summary>
            Retrieve first.
            </summary>
            <param name="expr">Expr.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.Add(System.String,System.Collections.Generic.Dictionary{System.String,System.Int32},System.Action{System.String,System.Int64,System.Decimal},System.Threading.CancellationToken)">
            <summary>
            Add.
            </summary>
            <param name="name">Model name.</param>
            <param name="quantizationPriority">Quantization priority.</param>
            <param name="progressCallback">Progress callback (URL, bytes downloaded, percentage 0-1).</param>
            <param name="token">Cancellation token.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.ModelDriver.Delete(System.String)">
            <summary>
            Delete.
            </summary>
            <param name="name">Model name.</param>
        </member>
        <member name="M:SharpAI.ModelDriver.GetEngine(System.String)">
            <summary>
            Get the engine for a given model.
            </summary>
            <param name="name">Model name.</param>
            <returns>Engine.</returns>
        </member>
        <member name="T:SharpAI.Models.EnumerationOrderEnum">
            <summary>
            Enumeration order.
            </summary>
        </member>
        <member name="F:SharpAI.Models.EnumerationOrderEnum.CreatedAscending">
            <summary>
            CreatedAscending.
            </summary>
        </member>
        <member name="F:SharpAI.Models.EnumerationOrderEnum.CreatedDescending">
            <summary>
            CreatedDescending.
            </summary>
        </member>
        <member name="F:SharpAI.Models.EnumerationOrderEnum.SizeAscending">
            <summary>
            SizeAscending.
            </summary>
        </member>
        <member name="F:SharpAI.Models.EnumerationOrderEnum.SizeDescending">
            <summary>
            SizeDescending.
            </summary>
        </member>
        <member name="F:SharpAI.Models.EnumerationOrderEnum.NameAscending">
            <summary>
            NameAscending.
            </summary>
        </member>
        <member name="F:SharpAI.Models.EnumerationOrderEnum.NameDescending">
            <summary>
            KeyDescending.
            </summary>
        </member>
        <member name="T:SharpAI.Models.EnumerationResult`1">
            <summary>
            Object returned as the result of an enumeration.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.Success">
            <summary>
            Indicates if the statistics operation was successful.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.Timestamp">
            <summary>
            Start and end timestamps.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.MaxResults">
            <summary>
            Maximum number of results to retrieve.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.Skip">
            <summary>
            Skip.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.IterationsRequired">
            <summary>
            Iterations required.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.ContinuationToken">
            <summary>
            Continuation token.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.EndOfResults">
            <summary>
            Boolean indicating end of results.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.TotalRecords">
            <summary>
            Total number of records.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.RecordsRemaining">
            <summary>
            Number of candidate records remaining in the enumeration.
            </summary>
        </member>
        <member name="P:SharpAI.Models.EnumerationResult`1.Objects">
            <summary>
            Objects.
            </summary>
        </member>
        <member name="M:SharpAI.Models.EnumerationResult`1.#ctor">
            <summary>
            Instantiates the object.
            </summary>
        </member>
        <member name="T:SharpAI.Models.ModelFile">
            <summary>
            Metadata about a model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.Id">
            <summary>
            ID.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.GUID">
            <summary>
            GUID.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.Name">
            <summary>
            Name.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.ParentModel">
            <summary>
            Parent model name.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.Format">
            <summary>
            Model format.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.Family">
            <summary>
            Model family.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.ContentLength">
            <summary>
            Content length of the file.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.ParameterCount">
            <summary>
            Parameter count.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.MD5Hash">
            <summary>
            MD5.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.SHA1Hash">
            <summary>
            SHA1.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.SHA256Hash">
            <summary>
            SHA256.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.SourceUrl">
            <summary>
            Source URL.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.ParameterSize">
            <summary>
            Parameter size.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.Quantization">
            <summary>
            Quantization.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.Embeddings">
            <summary>
            Boolean indicating if the model can be used for embeddings.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.Completions">
            <summary>
            Boolean indicating if the model can be used for completions.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.ModelCreationUtc">
            <summary>
            Timestamp from the hosting provider, generally a last modified timestamp, in UTC time.
            </summary>
        </member>
        <member name="P:SharpAI.Models.ModelFile.CreatedUtc">
            <summary>
            Timestamp from creation, in UTC time.
            </summary>
        </member>
        <member name="M:SharpAI.Models.ModelFile.#ctor">
            <summary>
            Metadata about a model.
            </summary>
        </member>
        <member name="M:SharpAI.Models.ModelFile.ToOllamaModelDetails">
            <summary>
            Convert to an Ollama model details API object.
            </summary>
            <returns>Object.</returns>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaChatCompletionMetrics">
            <summary>
            Performance metrics for chat completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.PromptTokens">
            <summary>
            Number of tokens in the prompt.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.CompletionTokens">
            <summary>
            Number of tokens generated.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.TotalTokens">
            <summary>
            Total number of tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.PromptTokensPerSecond">
            <summary>
            Prompt evaluation rate (tokens/second).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.GenerationTokensPerSecond">
            <summary>
            Generation rate (tokens/second).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.TotalDurationMs">
            <summary>
            Total response time in milliseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.LoadDurationMs">
            <summary>
            Model load time in milliseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.PromptEvalDurationMs">
            <summary>
            Prompt evaluation time in milliseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.GenerationDurationMs">
            <summary>
            Generation time in milliseconds.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.FromResult(SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult)">
            <summary>
            Creates metrics from a chat completion result.
            </summary>
            <param name="result">The chat completion result.</param>
            <returns>Metrics object or null if insufficient data.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaChatCompletionMetrics.#ctor">
            <summary>
            Ollama chat completion metrics.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaChatMessage">
            <summary>
            Ollama chat message.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatMessage.Role">
            <summary>
            Role of the message sender (required).
            Valid values: "system", "user", "assistant", "tool"
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatMessage.Content">
            <summary>
            Content of the message (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatMessage.Images">
            <summary>
            Base64-encoded images for multimodal models (optional).
            Only valid for user messages.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaChatMessage.ToolCalls">
            <summary>
            Tool calls made by the assistant (optional).
            Only valid for assistant messages.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaChatMessage.#ctor">
            <summary>
            Ollama chat message.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaChatResponseMessage">
            <summary>
            Extension of OllamaChatMessage for response-specific properties.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaChatResponseMessage.IsToolCallResponse">
            <summary>
            Tool calls made by the assistant (if any).
            Inherited from OllamaChatMessage.
            </summary>
            <summary>
            Checks if this is a tool call response.
            </summary>
            <returns>True if the message contains tool calls.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaChatResponseMessage.GetFirstToolCall">
            <summary>
            Gets the first tool call if available.
            </summary>
            <returns>First tool call or null if none.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaChatResponseMessage.#ctor">
            <summary>
            Ollama chat response message.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaCompletionContext">
            <summary>
            Helper class for managing completion context across requests.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionContext.Data">
            <summary>
            The context data.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionContext.Model">
            <summary>
            The model this context is for.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionContext.LastUpdated">
            <summary>
            When this context was last updated.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaCompletionContext.FromResult(SharpAI.Models.Ollama.OllamaGenerateCompletionResult)">
            <summary>
            Creates a context from a completion result.
            </summary>
            <param name="result">The completion result.</param>
            <returns>Context object or null if no context available.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaCompletionContext.IsValidForModel(System.String)">
            <summary>
            Checks if this context is valid for a given model.
            </summary>
            <param name="modelName">The model name to check.</param>
            <returns>True if context is valid for the model.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaCompletionContext.GetAge">
            <summary>
            Gets the age of this context.
            </summary>
            <returns>Time since last update.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaCompletionContext.#ctor">
            <summary>
            Ollama completion context.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaCompletionMetrics">
            <summary>
            Performance metrics for completion generation.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.PromptTokens">
            <summary>
            Number of tokens in the prompt.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.CompletionTokens">
            <summary>
            Number of tokens generated.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.TotalTokens">
            <summary>
            Total number of tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.ContextSize">
            <summary>
            Size of the context maintained.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.PromptTokensPerSecond">
            <summary>
            Prompt evaluation rate (tokens/second).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.GenerationTokensPerSecond">
            <summary>
            Generation rate (tokens/second).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.TotalDurationMs">
            <summary>
            Total response time in milliseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.LoadDurationMs">
            <summary>
            Model load time in milliseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.PromptEvalDurationMs">
            <summary>
            Prompt evaluation time in milliseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionMetrics.GenerationDurationMs">
            <summary>
            Generation time in milliseconds.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaCompletionMetrics.FromResult(SharpAI.Models.Ollama.OllamaGenerateCompletionResult)">
            <summary>
            Creates metrics from a completion result.
            </summary>
            <param name="result">The completion result.</param>
            <returns>Metrics object or null if insufficient data.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaCompletionMetrics.#ctor">
            <summary>
            Ollama completion metrics.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaCompletionOptions">
            <summary>
            Ollama completion options for model parameters.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.Seed">
            <summary>
            Random seed for generation (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.NumPredict">
            <summary>
            Number of tokens to generate (optional, default: infinite).
            -1 = infinite, -2 = fill context
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.NumGpu">
            <summary>
            Number of layers to offload to GPU (optional).
            0 = no GPU, -1 = all layers
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.Temperature">
            <summary>
            Temperature for sampling (optional, default: 0.8).
            Range: 0.0 to 2.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.TopK">
            <summary>
            Top-k sampling parameter (optional, default: 40).
            Range: 0 to 100
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.TopP">
            <summary>
            Top-p sampling parameter (optional, default: 0.9).
            Range: 0.0 to 1.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.MinP">
            <summary>
            Min-p sampling parameter (optional, default: 0.0).
            Range: 0.0 to 1.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.TfsZ">
            <summary>
            Tail free sampling parameter (optional, default: 1.0).
            Range: 0.0 to 1.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.TypicalP">
            <summary>
            Typical sampling parameter (optional, default: 1.0).
            Range: 0.0 to 1.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.RepeatPenalty">
            <summary>
            Repeat penalty (optional, default: 1.1).
            Range: 0.0 to 2.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.RepeatLastN">
            <summary>
            Last n tokens to consider for repeat penalty (optional, default: 64).
            Range: 0 to context size
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.PresencePenalty">
            <summary>
            Presence penalty (optional, default: 0.0).
            Range: -2.0 to 2.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.FrequencyPenalty">
            <summary>
            Frequency penalty (optional, default: 0.0).
            Range: -2.0 to 2.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.Mirostat">
            <summary>
            Mirostat sampling mode (0/1/2) (optional, default: 0).
            0 = disabled, 1 = Mirostat v1, 2 = Mirostat v2
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.MirostatTau">
            <summary>
            Mirostat target entropy tau (optional, default: 5.0).
            Range: 0.0 to 10.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.MirostatEta">
            <summary>
            Mirostat learning rate eta (optional, default: 0.1).
            Range: 0.0 to 1.0
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.PenalizeNewline">
            <summary>
            Penalize newline tokens (optional, default: true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.Stop">
            <summary>
            Stop sequences for generation (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.Numa">
            <summary>
            Enable NUMA support (optional, default: false).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.NumCtx">
            <summary>
            Context size (optional, default: 2048).
            Range: 128 to 1048576
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.NumBatch">
            <summary>
            Batch size for prompt evaluation (optional, default: 512).
            Range: 1 to context size
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.NumThread">
            <summary>
            Number of threads to use (optional).
            Range: 1 to 128
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.NumKeep">
            <summary>
            Number of tokens to keep from initial prompt (optional, default: 4).
            Range: 0 to context size
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.UseMlock">
            <summary>
            Use memory locking (optional, default: false).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.UseMmap">
            <summary>
            Use memory mapping (optional, default: true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.VocabOnly">
            <summary>
            Vocabulary only mode (optional, default: false).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.LowVram">
            <summary>
            Low VRAM mode (optional, default: false).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.F16Kv">
            <summary>
            F16 key-value storage (optional, default: true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.MainGpu">
            <summary>
            Main GPU index (optional).
            Range: 0 to number of GPUs - 1
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaCompletionOptions.LogitsAll">
            <summary>
            Logits all mode (optional).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaCompletionOptions.#ctor">
            <summary>
            Ollama completion options.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaDeleteModelRequest">
            <summary>
            Ollama delete model request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaDeleteModelRequest.Model">
            <summary>
            Name of the model to delete (required).
            No one knows why Ollama chose to use the 'name' property here instead of 'model', which is used by the other APIs.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaDeleteModelRequest.#ctor">
            <summary>
            Ollama delete model request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaEmbeddingsInputConverter">
            <summary>
            Custom JSON converter for flexible input handling (string or array of strings).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaEmbeddingsInputConverter.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Read.
            </summary>
            <param name="reader">Reader.</param>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">Options.</param>
            <returns>Object.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaEmbeddingsInputConverter.Write(System.Text.Json.Utf8JsonWriter,System.Object,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Write.
            </summary>
            <param name="writer">Writer.</param>
            <param name="value">Value.</param>
            <param name="options">Options.</param>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaEmbeddingsOptions">
            <summary>
            Ollama embeddings options.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.NumThread">
            <summary>
            Number of threads to use (optional).
            Range: 1 to 128
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.NumCtx">
            <summary>
            Context size (optional).
            Range: 128 to 1048576
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.NumGpu">
            <summary>
            Number of layers to offload to GPU (optional).
            0 = no GPU, -1 = all layers
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.MainGpu">
            <summary>
            Main GPU index (optional).
            Range: 0 to number of GPUs - 1
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.LowVram">
            <summary>
            Low VRAM mode (optional, default: false).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.F16Kv">
            <summary>
            F16 key-value storage (optional, default: true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.Numa">
            <summary>
            Enable NUMA support (optional, default: false).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.UseMmap">
            <summary>
            Use memory mapping (optional, default: true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.UseMlock">
            <summary>
            Use memory locking (optional, default: false).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaEmbeddingsOptions.#ctor">
            <summary>
            Ollama embeddings options.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaEmbeddingsOutputConverter">
            <summary>
            Custom JSON converter for flexible embeddings handling (single array or array of arrays).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaEmbeddingsOutputConverter.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Read.
            </summary>
            <param name="reader">Reader.</param>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">Options.</param>
            <returns>Object.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaEmbeddingsOutputConverter.Write(System.Text.Json.Utf8JsonWriter,System.Object,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Write.
            </summary>
            <param name="writer">Writer.</param>
            <param name="value">Value.</param>
            <param name="options">Options.</param>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk">
            <summary>
            Ollama generate chat completion chunk (single chunk from streaming response).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.Model">
            <summary>
            The model that generated the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.CreatedAt">
            <summary>
            The timestamp of when the response was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.Message">
            <summary>
            The partial message generated by the model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.Done">
            <summary>
            Whether the response is complete.
            False for intermediate chunks, true for the final chunk.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.DoneReason">
            <summary>
            Reason why the response is done.
            Examples: "stop", "length", "function_call"
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.TotalDuration">
            <summary>
            Total duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.LoadDuration">
            <summary>
            Model load duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.PromptEvalCount">
            <summary>
            Number of tokens in the prompt.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.PromptEvalDuration">
            <summary>
            Prompt evaluation duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.EvalCount">
            <summary>
            Number of tokens generated in the response.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.EvalDuration">
            <summary>
            Response generation duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionChunk.#ctor">
            <summary>
            Ollama generate chat completion chunk.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest">
            <summary>
            Ollama generate chat completion request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.Model">
            <summary>
            Model name to use for chat completion (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.Messages">
            <summary>
            Messages for the chat (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.Options">
            <summary>
            Additional model parameters (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.Format">
            <summary>
            Format to return the response in. Currently only "json" is supported (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.Template">
            <summary>
            The full prompt template (overrides what is defined in the Modelfile) (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.Stream">
            <summary>
            Enable streaming of generated text (optional, defaults to true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.KeepAlive">
            <summary>
            How long to keep the model loaded in memory (optional).
            Examples: "5m", "10m", "1h", "never"
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.Tools">
            <summary>
            Tools/functions available for the model to use (optional).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionRequest.#ctor">
            <summary>
            Ollama generate chat completion request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult">
            <summary>
            Ollama generate chat completion result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.Model">
            <summary>
            The model that generated the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.CreatedAt">
            <summary>
            The timestamp of when the response was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.Message">
            <summary>
            The message generated by the model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.Done">
            <summary>
            Whether the response is complete.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.DoneReason">
            <summary>
            Reason why the response is done.
            Examples: "stop", "length", "function_call"
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.TotalDuration">
            <summary>
            Total duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.LoadDuration">
            <summary>
            Model load duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.PromptEvalCount">
            <summary>
            Number of tokens in the prompt.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.PromptEvalDuration">
            <summary>
            Prompt evaluation duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.EvalCount">
            <summary>
            Number of tokens generated in the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.EvalDuration">
            <summary>
            Response generation duration in nanoseconds.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.GetPromptTokensPerSecond">
            <summary>
            Gets the prompt evaluation rate in tokens per second.
            </summary>
            <returns>Tokens per second for prompt evaluation, or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.GetGenerationTokensPerSecond">
            <summary>
            Gets the response generation rate in tokens per second.
            </summary>
            <returns>Tokens per second for generation, or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.GetTotalDurationMilliseconds">
            <summary>
            Gets the total response time in milliseconds.
            </summary>
            <returns>Total duration in milliseconds, or null if unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.GetLoadDurationMilliseconds">
            <summary>
            Gets the model load time in milliseconds.
            </summary>
            <returns>Load duration in milliseconds, or null if unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.HasToolCalls">
            <summary>
            Checks if the response contains tool calls.
            </summary>
            <returns>True if the message contains tool calls.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.GetTotalTokenCount">
            <summary>
            Gets the total number of tokens (prompt + generated).
            </summary>
            <returns>Total token count, or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionResult.#ctor">
            <summary>
            Ollama generate chat completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateChatCompletionStreamingResult">
            <summary>
            Ollama generate chat completion streaming result wrapper.
            This wrapper is returned immediately and contains an async enumerable stream of chunks.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateChatCompletionStreamingResult.Chunks">
            <summary>
            The async enumerable stream of chat completion chunks.
            Yields OllamaGenerateChatCompletionChunk objects as they arrive from the server.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateChatCompletionStreamingResult.#ctor">
            <summary>
            Ollama generate chat completion streaming result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk">
            <summary>
            Ollama generate completion chunk (single chunk from streaming response).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.Model">
            <summary>
            The model that generated the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.CreatedAt">
            <summary>
            The timestamp of when the response was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.Response">
            <summary>
            The partial generated text response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.Done">
            <summary>
            Whether the response is complete.
            False for intermediate chunks, true for the final chunk.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.Context">
            <summary>
            Context for maintaining conversation state.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.TotalDuration">
            <summary>
            Total duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.LoadDuration">
            <summary>
            Model load duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.PromptEvalCount">
            <summary>
            Number of tokens in the prompt.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.PromptEvalDuration">
            <summary>
            Prompt evaluation duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.EvalCount">
            <summary>
            Number of tokens generated in the response.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.EvalDuration">
            <summary>
            Response generation duration in nanoseconds.
            Only present in the final chunk (when done=true).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionChunk.#ctor">
            <summary>
            Ollama generate completion chunk.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest">
            <summary>
            Ollama generate completion request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Model">
            <summary>
            Model name to use for generation (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Prompt">
            <summary>
            The prompt to generate a response for (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Options">
            <summary>
            Additional model parameters (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.System">
            <summary>
            System message to use (overrides what is defined in the Modelfile) (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Template">
            <summary>
            The full prompt or prompt template (overrides what is defined in the Modelfile) (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Context">
            <summary>
            The context from a previous request, used to keep a conversation going (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Stream">
            <summary>
            Enable streaming of generated text (optional, defaults to true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Raw">
            <summary>
            If false, the response will not include the prompt (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Format">
            <summary>
            Format to return the response in. Currently only "json" is supported (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.Images">
            <summary>
            Base64-encoded images for multimodal models (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.KeepAlive">
            <summary>
            How long to keep the model loaded in memory (optional).
            Examples: "5m", "10m", "1h", "never"
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionRequest.#ctor">
            <summary>
            Ollama generate completion request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateCompletionResult">
            <summary>
            Ollama generate completion result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.Model">
            <summary>
            The model that generated the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.CreatedAt">
            <summary>
            The timestamp of when the response was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.Response">
            <summary>
            The generated text response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.Context">
            <summary>
            Context for maintaining conversation state.
            Can be passed in the next request to continue the conversation.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.TotalDuration">
            <summary>
            Total duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.LoadDuration">
            <summary>
            Model load duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.PromptEvalCount">
            <summary>
            Number of tokens in the prompt.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.PromptEvalDuration">
            <summary>
            Prompt evaluation duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.EvalCount">
            <summary>
            Number of tokens generated in the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.EvalDuration">
            <summary>
            Response generation duration in nanoseconds.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetPromptTokensPerSecond">
            <summary>
            Gets the prompt evaluation rate in tokens per second.
            </summary>
            <returns>Tokens per second for prompt evaluation, or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetGenerationTokensPerSecond">
            <summary>
            Gets the response generation rate in tokens per second.
            </summary>
            <returns>Tokens per second for generation, or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetTotalDurationMilliseconds">
            <summary>
            Gets the total response time in milliseconds.
            </summary>
            <returns>Total duration in milliseconds, or null if unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetLoadDurationMilliseconds">
            <summary>
            Gets the model load time in milliseconds.
            </summary>
            <returns>Load duration in milliseconds, or null if unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetPromptEvalDurationMilliseconds">
            <summary>
            Gets the prompt evaluation time in milliseconds.
            </summary>
            <returns>Prompt evaluation duration in milliseconds, or null if unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetEvalDurationMilliseconds">
            <summary>
            Gets the generation time in milliseconds.
            </summary>
            <returns>Generation duration in milliseconds, or null if unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetTotalTokenCount">
            <summary>
            Gets the total number of tokens (prompt + generated).
            </summary>
            <returns>Total token count, or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.HasContext">
            <summary>
            Checks if context is available for continuing the conversation.
            </summary>
            <returns>True if context is available.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.GetContextSize">
            <summary>
            Gets the size of the context in tokens.
            </summary>
            <returns>Context size or 0 if no context.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionResult.#ctor">
            <summary>
            Ollama generate completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateCompletionStreamingResult">
            <summary>
            Ollama generate completion streaming result wrapper.
            This wrapper is returned immediately and contains an async enumerable stream of chunks.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateCompletionStreamingResult.Chunks">
            <summary>
            The async enumerable stream of completion chunks.
            Yields OllamaGenerateCompletionChunk objects as they arrive from the server.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateCompletionStreamingResult.#ctor">
            <summary>
            Ollama generate completion streaming result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest">
            <summary>
            Ollama generate embeddings request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.Model">
            <summary>
            Model name to use for generating embeddings (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.Input">
            <summary>
            Input text(s) to generate embeddings for (required).
            Can be a single string or an array of strings.
            Use SetInput() or SetInputs() to set values, and GetInputs() to retrieve as array.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.Options">
            <summary>
            Options for generating embeddings (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.KeepAlive">
            <summary>
            How long to keep the model loaded in memory (optional).
            Examples: "5m", "10m", "1h", "never"
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.Truncate">
            <summary>
            Truncate inputs that exceed the model's context window (optional).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.SetInput(System.String)">
            <summary>
            Sets a single input string.
            </summary>
            <param name="input">The input string.</param>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.SetInputs(System.Collections.Generic.List{System.String})">
            <summary>
            Sets multiple input strings.
            </summary>
            <param name="inputs">The array of input strings.</param>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.GetInput">
            <summary>
            Gets the input as a single string.
            Throws an exception if the input is a list.
            </summary>
            <returns>The input string.</returns>
            <exception cref="T:System.InvalidOperationException">Thrown when input is a list instead of a single string.</exception>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.GetInputs">
            <summary>
            Gets the input as an array of strings.
            If input is a single string, returns an array with one element.
            </summary>
            <returns>Array of input strings.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.IsSingleInput">
            <summary>
            Checks if the input is a single string.
            </summary>
            <returns>True if input is a single string, false otherwise.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsRequest.#ctor">
            <summary>
            Ollama generate embeddings request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult">
            <summary>
            Ollama generate embeddings result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.Model">
            <summary>
            The model that generated the embeddings.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.Embeddings">
            <summary>
            Generated embedding(s).
            Can be a single array of floats or an array of arrays of floats.
            Use GetEmbedding() for single result or GetEmbeddings() for multiple results.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.TotalDuration">
            <summary>
            Total duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.LoadDuration">
            <summary>
            Model load duration in nanoseconds.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.PromptEvalCount">
            <summary>
            Prompt evaluation count.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.GetEmbedding">
            <summary>
            Gets a single embedding array.
            Throws if the result contains multiple embeddings.
            </summary>
            <returns>Single embedding array.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.GetEmbeddings">
            <summary>
            Gets all embeddings as a list of arrays.
            If result is a single embedding, returns a list with one element.
            </summary>
            <returns>List of embedding arrays.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.IsSingleEmbedding">
            <summary>
            Checks if the result contains a single embedding.
            </summary>
            <returns>True if single embedding, false if multiple.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.IsMultiEmbeddings">
            <summary>
            Checks if the result contains multiple embeddings.
            </summary>
            <returns>True if multiple embeddings, false if single.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.GetEmbeddingCount">
            <summary>
            Gets the number of embeddings in the result.
            </summary>
            <returns>Number of embeddings.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.GetEmbeddingDimension">
            <summary>
            Gets the dimension (length) of the embeddings.
            All embeddings in the result should have the same dimension.
            </summary>
            <returns>Dimension of embeddings, or null if no embeddings.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaGenerateEmbeddingsResult.#ctor">
            <summary>
            Ollama generate embeddings result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaListLocalModelsResult">
            <summary>
            Ollama list local models result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaListLocalModelsResult.Models">
            <summary>
            List of models available locally.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetModelCount">
            <summary>
            Gets the total number of models.
            </summary>
            <returns>Number of local models.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetTotalSizeBytes">
            <summary>
            Gets the total size of all models in bytes.
            </summary>
            <returns>Total size in bytes.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetFormattedTotalSize">
            <summary>
            Gets the total size of all models in a human-readable format.
            </summary>
            <returns>Formatted total size string.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.FindModel(System.String,System.Boolean)">
            <summary>
            Finds a model by name.
            </summary>
            <param name="modelName">The model name to search for.</param>
            <param name="exactMatch">Whether to require exact match or allow partial/case-insensitive match.</param>
            <returns>The matching model or null if not found.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetModelsByFamily(System.String)">
            <summary>
            Gets all models from a specific family.
            </summary>
            <param name="family">The model family (e.g., "llama", "mistral").</param>
            <returns>List of models from the specified family.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetQuantizedModels">
            <summary>
            Gets all quantized models.
            </summary>
            <returns>List of quantized models.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetModelsSortedBySize(System.Boolean)">
            <summary>
            Gets models sorted by size.
            </summary>
            <param name="ascending">True for smallest first, false for largest first.</param>
            <returns>Sorted list of models.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetModelsSortedByDate(System.Boolean)">
            <summary>
            Gets models sorted by modification date.
            </summary>
            <param name="mostRecentFirst">True for most recent first.</param>
            <returns>Sorted list of models.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetModelsModifiedSince(System.DateTime)">
            <summary>
            Gets models that were modified within a specific time period.
            </summary>
            <param name="since">The start date/time.</param>
            <returns>List of models modified since the specified time.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GroupModelsByBaseName">
            <summary>
            Groups models by their base name (without tag).
            </summary>
            <returns>Dictionary grouping models by base name.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.GetStatistics">
            <summary>
            Gets statistics about the local models.
            </summary>
            <returns>Model statistics.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListLocalModelsResult.#ctor">
            <summary>
            Ollama list local models result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaListRunningModelsResult">
            <summary>
            Ollama list running models result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaListRunningModelsResult.Models">
            <summary>
            List of models currently loaded in memory.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetRunningModelCount">
            <summary>
            Gets the total number of running models.
            </summary>
            <returns>Number of running models.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetTotalVRAMUsage">
            <summary>
            Gets the total VRAM usage of all running models in bytes.
            </summary>
            <returns>Total VRAM usage in bytes.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetFormattedTotalVRAMUsage">
            <summary>
            Gets the total VRAM usage formatted as a string.
            </summary>
            <returns>Formatted VRAM usage.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetTotalRAMUsage">
            <summary>
            Gets the total system RAM usage of all running models in bytes.
            </summary>
            <returns>Total RAM usage in bytes.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetFormattedTotalRAMUsage">
            <summary>
            Gets the total system RAM usage formatted as a string.
            </summary>
            <returns>Formatted RAM usage.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetTotalMemoryUsage">
            <summary>
            Gets the total memory usage (VRAM + RAM) of all running models.
            </summary>
            <returns>Total memory usage in bytes.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetFormattedTotalMemoryUsage">
            <summary>
            Gets the total memory usage formatted as a string.
            </summary>
            <returns>Formatted total memory usage.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.FindRunningModel(System.String)">
            <summary>
            Finds a running model by name.
            </summary>
            <param name="modelName">The model name to search for.</param>
            <returns>The running model or null if not found.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.IsModelRunning(System.String)">
            <summary>
            Checks if a specific model is currently running.
            </summary>
            <param name="modelName">The model name to check.</param>
            <returns>True if the model is running.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetModelsExpiringWithin(System.TimeSpan)">
            <summary>
            Gets models that will expire within a specified timeframe.
            </summary>
            <param name="within">Timespan to check for expiration.</param>
            <returns>List of models expiring within the timeframe.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetNextExpiringModel">
            <summary>
            Gets the model that will expire next.
            </summary>
            <returns>Model with the earliest expiration or null.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetModelsSortedByMemoryUsage(System.Boolean)">
            <summary>
            Gets models sorted by memory usage.
            </summary>
            <param name="ascending">True for smallest first, false for largest first.</param>
            <returns>Sorted list of running models.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetModelsSortedByVRAMUsage(System.Boolean)">
            <summary>
            Gets models sorted by VRAM usage.
            </summary>
            <param name="ascending">True for smallest first, false for largest first.</param>
            <returns>Sorted list of running models.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GroupByFamily">
            <summary>
            Groups running models by family.
            </summary>
            <returns>Dictionary of models grouped by family.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.GetStatistics">
            <summary>
            Gets memory usage statistics for running models.
            </summary>
            <returns>Memory usage statistics.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaListRunningModelsResult.#ctor">
            <summary>
            Ollama list running models result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaLocalModel">
            <summary>
            Ollama local model information.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaLocalModel.Name">
            <summary>
            Model name including tag (e.g., "llama2:latest").
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaLocalModel.Digest">
            <summary>
            Model digest/hash.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaLocalModel.Size">
            <summary>
            Model size in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaLocalModel.ModifiedAt">
            <summary>
            When the model was last modified.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaLocalModel.Details">
            <summary>
            Model details including format, family, parameter size, and quantization.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.GetBaseName">
            <summary>
            Gets the model's base name without tag.
            </summary>
            <returns>Base model name.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.GetTag">
            <summary>
            Gets the model's tag.
            </summary>
            <returns>Model tag or "latest" if no tag specified.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.GetFormattedSize">
            <summary>
            Gets the formatted size of the model.
            </summary>
            <returns>Human-readable size string.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.GetShortDigest">
            <summary>
            Gets a short digest identifier.
            </summary>
            <returns>First 12 characters of the digest.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.GetAge">
            <summary>
            Gets the age of the model since last modification.
            </summary>
            <returns>Time since last modification.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.GetFormattedAge">
            <summary>
            Gets a formatted age string.
            </summary>
            <returns>Human-readable age string.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.MatchesPattern(System.String)">
            <summary>
            Checks if this model matches a given pattern.
            </summary>
            <param name="pattern">Pattern to match (supports wildcards).</param>
            <returns>True if the model name matches the pattern.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaLocalModel.#ctor">
            <summary>
            Ollama local model.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaModelDetails">
            <summary>
            Ollama model details.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelDetails.ParentModel">
            <summary>
            Parent model this was created from.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelDetails.Format">
            <summary>
            Model format (e.g., "gguf").
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelDetails.Family">
            <summary>
            Model family (e.g., "llama", "mistral").
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelDetails.Families">
            <summary>
            Model families/architectures.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelDetails.ParameterSize">
            <summary>
            Parameter size (e.g., "7B", "13B", "70B").
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelDetails.QuantizationLevel">
            <summary>
            Quantization level (e.g., "Q4_0", "Q4_K_M", "Q8_0").
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelDetails.GetParameterSizeInBillions">
            <summary>
            Gets the estimated model size in billions of parameters.
            </summary>
            <returns>Number of billions of parameters, or null if cannot parse.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelDetails.GetQuantizationBits">
            <summary>
            Gets the quantization bits from the quantization level.
            </summary>
            <returns>Number of bits used for quantization, or null if cannot determine.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelDetails.IsQuantized">
            <summary>
            Checks if this is a quantized model.
            </summary>
            <returns>True if the model is quantized.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelDetails.BelongsToFamily(System.String)">
            <summary>
            Checks if this model belongs to a specific family.
            </summary>
            <param name="familyName">The family name to check.</param>
            <returns>True if the model belongs to the specified family.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelDetails.#ctor">
            <summary>
            Ollama model details.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaModelInfoInterpreter">
            <summary>
            Helper class for interpreting model information.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelInfoInterpreter.#ctor(SharpAI.Models.Ollama.OllamaShowModelInfoResult)">
            <summary>
            Creates a new model info interpreter.
            </summary>
            <param name="result">The model info result to interpret.</param>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelInfoInterpreter.EstimateVRAMRequirement">
            <summary>
            Gets the estimated VRAM requirement in GB.
            </summary>
            <returns>Estimated VRAM in GB, or null if cannot estimate.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelInfoInterpreter.GetContextLength">
            <summary>
            Gets the context length from parameters if available.
            </summary>
            <returns>Context length or null if not found.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelInfoInterpreter.GetTemperature">
            <summary>
            Gets the temperature setting from parameters if available.
            </summary>
            <returns>Temperature value or null if not found.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelInfoInterpreter.MeetsRequirements(System.Nullable{System.Double},System.Nullable{System.Int32})">
            <summary>
            Determines if this model is suitable for a given use case.
            </summary>
            <param name="requiredVRAM">Required VRAM in GB.</param>
            <param name="requiredContext">Required context length.</param>
            <returns>True if the model meets the requirements.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelInfoInterpreter.GetSummary">
            <summary>
            Gets a summary of the model's key characteristics.
            </summary>
            <returns>Summary string.</returns>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaModelStatistics">
            <summary>
            Statistics about local models.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.TotalModels">
            <summary>
            Total number of models.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.TotalSizeBytes">
            <summary>
            Total size of all models in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.TotalSizeFormatted">
            <summary>
            Total size formatted as string.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.AverageSizeBytes">
            <summary>
            Average model size in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.AverageSizeFormatted">
            <summary>
            Average model size formatted as string.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.LargestModel">
            <summary>
            Largest model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.SmallestModel">
            <summary>
            Smallest model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.NewestModel">
            <summary>
            Most recently modified model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.OldestModel">
            <summary>
            Oldest modified model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.ModelsByFamily">
            <summary>
            Count of models by family.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.ModelsByQuantization">
            <summary>
            Count of models by quantization level.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaModelStatistics.ModelsByParameterSize">
            <summary>
            Count of models by parameter size.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelStatistics.#ctor(SharpAI.Models.Ollama.OllamaListLocalModelsResult)">
            <summary>
            Creates statistics from a list result.
            </summary>
            <param name="result">The list models result.</param>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelStatistics.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaModelStatistics.GetSummary">
            <summary>
            Gets a summary of the statistics.
            </summary>
            <returns>Summary string.</returns>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaPullModelRequest">
            <summary>
            Ollama pull model request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelRequest.Model">
            <summary>
            Model name to pull (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelRequest.Insecure">
            <summary>
            Allow insecure connections to the registry (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelRequest.Stream">
            <summary>
            Enable streaming of pull progress (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelRequest.Username">
            <summary>
            Username for registry authentication (optional).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelRequest.Password">
            <summary>
            Password for registry authentication (optional).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelRequest.#ctor">
            <summary>
            Ollama pull model request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaPullModelResultMessage">
            <summary>
            Ollama pull model result message for streaming updates during model download.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelResultMessage.Status">
            <summary>
            Current status of the pull operation.
            Examples: "pulling manifest", "downloading", "verifying sha256 digest", "writing manifest", "removing any unused layers", "success"
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelResultMessage.Digest">
            <summary>
            Digest of the layer being downloaded.
            Format: "sha256:hash"
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelResultMessage.Total">
            <summary>
            Total size of the current layer in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelResultMessage.Completed">
            <summary>
            Number of bytes completed for the current layer.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullModelResultMessage.Error">
            <summary>
            Error message if the pull operation failed.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.GetProgressPercentage">
            <summary>
            Gets the download progress as a percentage.
            </summary>
            <returns>Progress percentage (0-100), or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.GetRemainingBytes">
            <summary>
            Gets the remaining bytes to download.
            </summary>
            <returns>Remaining bytes, or null if data unavailable.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.GetFormattedTotalSize">
            <summary>
            Formats the total size in a human-readable format.
            </summary>
            <returns>Formatted size string (e.g., "1.5 GB").</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.GetFormattedCompletedSize">
            <summary>
            Formats the completed size in a human-readable format.
            </summary>
            <returns>Formatted size string (e.g., "750 MB").</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.GetFormattedProgress">
            <summary>
            Gets a formatted progress string.
            </summary>
            <returns>Progress string (e.g., "750 MB / 1.5 GB (50%)").</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.IsDownloadProgress">
            <summary>
            Checks if this is a download progress message.
            </summary>
            <returns>True if this message contains download progress information.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.IsComplete">
            <summary>
            Checks if the operation is complete.
            </summary>
            <returns>True if the status indicates completion.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.HasError">
            <summary>
            Checks if the operation has failed.
            </summary>
            <returns>True if an error occurred.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.IsStatusMessage">
            <summary>
            Checks if this is a status-only message (no download progress).
            </summary>
            <returns>True if this is a status message without progress data.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.GetLayerId">
            <summary>
            Gets the layer identifier from the digest.
            </summary>
            <returns>Short form of the digest (first 12 characters of hash), or null if no digest.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.EstimateTimeRemaining(System.Double)">
            <summary>
            Estimates time remaining based on a given download rate.
            </summary>
            <param name="bytesPerSecond">Current download rate in bytes per second.</param>
            <returns>Estimated time remaining, or null if cannot be calculated.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
            <param name="bytes">Number of bytes.</param>
            <returns>Formatted string (e.g., "1.5 GB").</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullModelResultMessage.#ctor">
            <summary>
            Ollama pull model result message.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaPullProgressTracker">
            <summary>
            Helper class for tracking pull operation progress across multiple messages.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.TotalBytes">
            <summary>
            Total bytes to download across all layers.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.CompletedBytes">
            <summary>
            Total bytes completed across all layers.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.CurrentStatus">
            <summary>
            Current status message.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.CurrentLayer">
            <summary>
            Current layer being downloaded.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.LayersCompleted">
            <summary>
            Number of layers completed.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.OverallProgress">
            <summary>
            Gets the overall progress percentage.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.DownloadRate">
            <summary>
            Gets the current download rate in bytes per second.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaPullProgressTracker.ElapsedTime">
            <summary>
            Gets the elapsed time since the pull started.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullProgressTracker.Update(SharpAI.Models.Ollama.OllamaPullModelResultMessage)">
            <summary>
            Updates the tracker with a new message.
            </summary>
            <param name="message">The pull result message.</param>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullProgressTracker.EstimateTimeRemaining">
            <summary>
            Estimates time remaining for the entire pull operation.
            </summary>
            <returns>Estimated time remaining, or null if cannot be calculated.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullProgressTracker.GetSummary">
            <summary>
            Gets a formatted summary of the pull progress.
            </summary>
            <returns>Summary string.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullProgressTracker.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullProgressTracker.FormatTimeSpan(System.TimeSpan)">
            <summary>
            Formats a timespan into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaPullProgressTracker.#ctor">
            <summary>
            Creates a new pull progress tracker.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaRunningModel">
            <summary>
            Ollama running model information.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModel.Name">
            <summary>
            Model name including tag.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModel.Digest">
            <summary>
            Model digest/hash.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModel.Size">
            <summary>
            Total memory size in bytes (RAM + VRAM).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModel.SizeVRAM">
            <summary>
            VRAM usage in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModel.ExpiresAt">
            <summary>
            When the model expires from memory.
            ISO 8601 format string.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModel.Details">
            <summary>
            Model details including format, family, parameter size, and quantization.
            Reusing the existing OllamaModelDetails class.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetRAMUsage">
            <summary>
            Gets the system RAM usage (total size minus VRAM).
            </summary>
            <returns>RAM usage in bytes.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetFormattedSize">
            <summary>
            Gets the formatted total memory size.
            </summary>
            <returns>Human-readable size string.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetFormattedVRAMSize">
            <summary>
            Gets the formatted VRAM usage.
            </summary>
            <returns>Human-readable VRAM size string.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetFormattedRAMSize">
            <summary>
            Gets the formatted RAM usage.
            </summary>
            <returns>Human-readable RAM size string.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetVRAMPercentage">
            <summary>
            Gets the percentage of memory in VRAM vs RAM.
            </summary>
            <returns>Percentage of memory in VRAM (0-100).</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetTimeUntilExpiration">
            <summary>
            Gets time until the model expires from memory.
            </summary>
            <returns>Time remaining or null if no expiration set.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetFormattedTimeUntilExpiration">
            <summary>
            Gets formatted time until expiration.
            </summary>
            <returns>Human-readable time remaining.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.HasExpired">
            <summary>
            Checks if the model has expired.
            </summary>
            <returns>True if the model has expired.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetEstimatedExpiration">
            <summary>
            Gets estimated expiration time based on expires_at field.
            </summary>
            <returns>Estimated expiration DateTime or null.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetBaseName">
            <summary>
            Gets the model's base name without tag.
            </summary>
            <returns>Base model name.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetTag">
            <summary>
            Gets the model's tag.
            </summary>
            <returns>Model tag or "latest" if no tag specified.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.GetMemoryUsageSummary">
            <summary>
            Gets a memory usage summary.
            </summary>
            <returns>Summary string of memory usage.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModel.#ctor">
            <summary>
            Ollama running model.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaRunningModelStatistics">
            <summary>
            Statistics about running models.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.TotalRunningModels">
            <summary>
            Total number of running models.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.TotalMemoryBytes">
            <summary>
            Total memory usage in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.TotalMemoryFormatted">
            <summary>
            Total memory usage formatted.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.TotalVRAMBytes">
            <summary>
            Total VRAM usage in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.TotalVRAMFormatted">
            <summary>
            Total VRAM usage formatted.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.TotalRAMBytes">
            <summary>
            Total RAM usage in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.TotalRAMFormatted">
            <summary>
            Total RAM usage formatted.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.AverageMemoryBytes">
            <summary>
            Average memory per model in bytes.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.AverageMemoryFormatted">
            <summary>
            Average memory per model formatted.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.LargestModel">
            <summary>
            Model using the most memory.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.SmallestModel">
            <summary>
            Model using the least memory.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.NextToExpire">
            <summary>
            Model expiring soonest.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.ModelsUsingVRAM">
            <summary>
            Number of models with VRAM allocation.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.AverageVRAMPercentage">
            <summary>
            Average VRAM percentage across models.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaRunningModelStatistics.ModelsByFamily">
            <summary>
            Count of running models by family.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModelStatistics.#ctor(SharpAI.Models.Ollama.OllamaListRunningModelsResult)">
            <summary>
            Creates statistics from running models result.
            </summary>
            <param name="result">The running models result.</param>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModelStatistics.FormatBytes(System.Int64)">
            <summary>
            Formats bytes into human-readable format.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaRunningModelStatistics.GetSummary">
            <summary>
            Gets a summary of the statistics.
            </summary>
            <returns>Summary string.</returns>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaShowModelInfoRequest">
            <summary>
            Ollama show model information request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoRequest.Model">
            <summary>
            Name of the model to show (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoRequest.Verbose">
            <summary>
            Include verbose information about the model (optional).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoRequest.#ctor">
            <summary>
            Ollama show model information request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaShowModelInfoResult">
            <summary>
            Ollama show model information result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.License">
            <summary>
            Model license information.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.Modelfile">
            <summary>
            Model file content (when verbose is true).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.Parameters">
            <summary>
            Model parameters configuration.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.Template">
            <summary>
            Template used for prompts.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.System">
            <summary>
            System message/prompt.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.Details">
            <summary>
            Model details including format, family, families, parameter size, and quantization level.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.Messages">
            <summary>
            Messages/examples used in the model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.ModelInfo">
            <summary>
            Model information including general metadata.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaShowModelInfoResult.ModifiedAt">
            <summary>
            Modified timestamp.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoResult.IsVerboseResponse">
            <summary>
            Checks if this is a verbose response with full details.
            </summary>
            <returns>True if verbose information is included.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoResult.HasSystemPrompt">
            <summary>
            Checks if the model has a system prompt configured.
            </summary>
            <returns>True if system prompt is present.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoResult.HasExampleMessages">
            <summary>
            Checks if the model has example messages.
            </summary>
            <returns>True if example messages are present.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoResult.GetParameterSize">
            <summary>
            Gets the model's parameter size from details if available.
            </summary>
            <returns>Parameter size string (e.g., "7B", "13B") or null.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoResult.GetQuantizationLevel">
            <summary>
            Gets the model's quantization level from details if available.
            </summary>
            <returns>Quantization level string (e.g., "Q4_0", "Q8_0") or null.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoResult.ParseParameters">
            <summary>
            Parses the parameters string into a dictionary.
            </summary>
            <returns>Dictionary of parameter key-value pairs.</returns>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaShowModelInfoResult.#ctor">
            <summary>
            Ollama show model information result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaStreamingChatCompletionResult">
            <summary>
            Ollama streaming chat completion result.
            Used for intermediate streaming responses.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingChatCompletionResult.Model">
            <summary>
            The model that generated the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingChatCompletionResult.CreatedAt">
            <summary>
            The timestamp of when the response was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingChatCompletionResult.Message">
            <summary>
            The partial message generated by the model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingChatCompletionResult.Done">
            <summary>
            Whether this is the final chunk in the stream.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaStreamingChatCompletionResult.#ctor">
            <summary>
            Ollama streaming chat completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaStreamingCompletionResult">
            <summary>
            Ollama streaming completion result.
            Used for intermediate streaming responses.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingCompletionResult.Model">
            <summary>
            The model that generated the response.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingCompletionResult.CreatedAt">
            <summary>
            The timestamp of when the response was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingCompletionResult.Response">
            <summary>
            The partial response text.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaStreamingCompletionResult.Done">
            <summary>
            Whether this is the final chunk in the stream.
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaStreamingCompletionResult.#ctor">
            <summary>
            Ollama streaming completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaTool">
            <summary>
            Ollama tool definition.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaTool.Type">
            <summary>
            Type of tool (required).
            Currently only "function" is supported.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaTool.Function">
            <summary>
            Function definition (required when type is "function").
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaTool.#ctor">
            <summary>
            Ollama tool.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaToolCall">
            <summary>
            Ollama tool call made by the assistant.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolCall.Function">
            <summary>
            Function that was called (required).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaToolCall.#ctor">
            <summary>
            Ollama tool call.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaToolCallFunction">
            <summary>
            Ollama tool call function details.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolCallFunction.Name">
            <summary>
            Name of the function that was called (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolCallFunction.Arguments">
            <summary>
            Arguments passed to the function as a JSON string (required).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaToolCallFunction.#ctor">
            <summary>
            Ollama tool call function.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaToolFunction">
            <summary>
            Ollama tool function definition.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolFunction.Name">
            <summary>
            Name of the function (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolFunction.Description">
            <summary>
            Description of what the function does (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolFunction.Parameters">
            <summary>
            Parameters the function accepts, described as a JSON Schema object (required).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaToolFunction.#ctor">
            <summary>
            Ollama tool function.
            </summary>
        </member>
        <member name="T:SharpAI.Models.Ollama.OllamaToolParameters">
            <summary>
            Ollama tool parameters schema.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolParameters.Type">
            <summary>
            Type of the parameters object (required).
            Should typically be "object".
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolParameters.Properties">
            <summary>
            Properties of the parameters object (required).
            Each property is a JSON Schema definition.
            </summary>
        </member>
        <member name="P:SharpAI.Models.Ollama.OllamaToolParameters.Required">
            <summary>
            List of required property names (optional).
            </summary>
        </member>
        <member name="M:SharpAI.Models.Ollama.OllamaToolParameters.#ctor">
            <summary>
            Ollama tool parameters.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIChatChoice">
            <summary>
            OpenAI chat completion choice.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatChoice.Index">
            <summary>
            Index of this choice.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatChoice.Message">
            <summary>
            Chat message generated by the model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatChoice.Delta">
            <summary>
            Delta message for streaming responses.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatChoice.Logprobs">
            <summary>
            Log probabilities information.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatChoice.FinishReason">
            <summary>
            Reason the generation stopped.
            Possible values: "stop", "length", "tool_calls", "content_filter", "function_call"
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIChatChoice.#ctor">
            <summary>
            OpenAI chat choice.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk">
            <summary>
            OpenAI chat completion chunk (single chunk from streaming response).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.Id">
            <summary>
            Unique identifier for the chat completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.Object">
            <summary>
            Object type (always "chat.completion.chunk").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.Created">
            <summary>
            Unix timestamp when the completion was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.Model">
            <summary>
            Model used for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.SystemFingerprint">
            <summary>
            System fingerprint.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.Choices">
            <summary>
            List of chat completion choices with delta updates.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.Usage">
            <summary>
            Usage statistics for the completion.
            Only present in the final chunk when stream_options.include_usage is true.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIChatCompletionChunk.#ctor">
            <summary>
            OpenAI chat completion chunk.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIChatLogprobContent">
            <summary>
            OpenAI chat logprob content.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatLogprobContent.Token">
            <summary>
            The token.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatLogprobContent.Logprob">
            <summary>
            Log probability of this token.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatLogprobContent.Bytes">
            <summary>
            UTF-8 byte representation of the token.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatLogprobContent.TopLogprobs">
            <summary>
            Top alternative tokens and their log probabilities.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIChatLogprobContent.#ctor">
            <summary>
            OpenAI chat logprob content.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIChatLogprobs">
            <summary>
            OpenAI chat logprobs data.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatLogprobs.Content">
            <summary>
            Log probability information for each content token.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIChatLogprobs.#ctor">
            <summary>
            OpenAI chat logprobs.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIChatMessage">
            <summary>
            OpenAI chat message.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatMessage.Role">
            <summary>
            Role of the message sender (required).
            Valid values: "system", "user", "assistant", "tool"
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatMessage.Content">
            <summary>
            Content of the message.
            Can be a string or an array of content parts for multimodal input.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatMessage.Name">
            <summary>
            Name of the author of this message.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatMessage.ToolCalls">
            <summary>
            Tool calls made by the assistant.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIChatMessage.ToolCallId">
            <summary>
            Tool call ID this message is responding to.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIChatMessage.#ctor">
            <summary>
            OpenAI chat message.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAICompletionChoice">
            <summary>
            OpenAI completion choice.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChoice.Text">
            <summary>
            Generated text.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChoice.Index">
            <summary>
            Index of this choice.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChoice.Logprobs">
            <summary>
            Log probabilities information.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChoice.FinishReason">
            <summary>
            Reason the generation stopped.
            Possible values: "stop", "length"
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAICompletionChoice.#ctor">
            <summary>
            OpenAI completion choice.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAICompletionChunk">
            <summary>
            OpenAI completion chunk (single chunk from streaming response).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChunk.Id">
            <summary>
            Unique identifier for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChunk.Object">
            <summary>
            Object type (always "text_completion").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChunk.Created">
            <summary>
            Unix timestamp when the completion was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChunk.Model">
            <summary>
            Model used for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChunk.SystemFingerprint">
            <summary>
            System fingerprint.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionChunk.Choices">
            <summary>
            List of completion choices.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAICompletionChunk.#ctor">
            <summary>
            OpenAI completion chunk.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAICompletionTokensDetails">
            <summary>
            OpenAI completion tokens details.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionTokensDetails.ReasoningTokens">
            <summary>
            Number of reasoning tokens generated.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionTokensDetails.AudioTokens">
            <summary>
            Number of audio tokens generated.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionTokensDetails.AcceptedPredictionTokens">
            <summary>
            Number of tokens that were accepted.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAICompletionTokensDetails.RejectedPredictionTokens">
            <summary>
            Number of tokens that were rejected.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAICompletionTokensDetails.#ctor">
            <summary>
            OpenAI completion tokens details.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIEmbedding">
            <summary>
            OpenAI embedding data.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIEmbedding.Object">
            <summary>
            Object type (always "embedding").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIEmbedding.Index">
            <summary>
            Index of this embedding in the input array.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIEmbedding.Embedding">
            <summary>
            The embedding vector.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIEmbedding.#ctor">
            <summary>
            OpenAI embedding.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIEmbeddingsInputConverter">
            <summary>
            Converter for flexible embedding input (string or array).
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIEmbeddingUsage">
            <summary>
            OpenAI embedding usage statistics.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIEmbeddingUsage.PromptTokens">
            <summary>
            Number of tokens in the input.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIEmbeddingUsage.TotalTokens">
            <summary>
            Total number of tokens used.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIEmbeddingUsage.#ctor">
            <summary>
            OpenAI embedding usage.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIError">
            <summary>
            OpenAI error message.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIError.Error">
            <summary>
            Error details.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIError.#ctor">
            <summary>
            OpenAI error message.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIErrorDetails">
            <summary>
            OpenAI error details.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIErrorDetails.Message">
            <summary>
            Message.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIErrorDetails.Type">
            <summary>
            Type.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIErrorDetails.Parameters">
            <summary>
            Parameters.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIErrorDetails.Code">
            <summary>
            Code.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIErrorDetails.#ctor">
            <summary>
            OpenAI error details.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest">
            <summary>
            OpenAI generate chat completion request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Model">
            <summary>
            ID of the model to use (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Messages">
            <summary>
            Messages for the chat conversation (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.ResponseFormat">
            <summary>
            Response format specification.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.MaxTokens">
            <summary>
            The maximum number of tokens to generate.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Temperature">
            <summary>
            Temperature for sampling (0-2).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.TopP">
            <summary>
            Nucleus sampling parameter (0-1).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.N">
            <summary>
            Number of chat completions to generate for each input message.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Stream">
            <summary>
            Whether to stream partial progress.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Stop">
            <summary>
            Up to 4 sequences where the API will stop generating further tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.PresencePenalty">
            <summary>
            Presence penalty (-2.0 to 2.0).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.FrequencyPenalty">
            <summary>
            Frequency penalty (-2.0 to 2.0).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.LogitBias">
            <summary>
            Modify the likelihood of specified tokens appearing in the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Logprobs">
            <summary>
            Include the log probabilities on the logprobs most likely tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.TopLogprobs">
            <summary>
            Number of most likely tokens to return at each token position.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.User">
            <summary>
            A unique identifier representing your end-user.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Tools">
            <summary>
            Functions/tools the model may call.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.ToolChoice">
            <summary>
            Controls which (if any) tool is called by the model.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.ParallelToolCalls">
            <summary>
            Whether to enable parallel function calling.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.Seed">
            <summary>
            Random seed for deterministic generation.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionRequest.#ctor">
            <summary>
            OpenAI generate chat completion request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult">
            <summary>
            OpenAI generate chat completion result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.Id">
            <summary>
            Unique identifier for the chat completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.Object">
            <summary>
            Object type (always "chat.completion").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.Created">
            <summary>
            Unix timestamp when the completion was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.Model">
            <summary>
            Model used for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.SystemFingerprint">
            <summary>
            System fingerprint.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.Choices">
            <summary>
            List of chat completion choices.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.Usage">
            <summary>
            Usage statistics for the completion.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.GetCreatedDateTime">
            <summary>
            Gets the created timestamp as DateTime.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.GetAssistantMessage">
            <summary>
            Gets the primary message from the assistant.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.HasToolCalls">
            <summary>
            Checks if any choice contains tool calls.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.GetAllToolCalls">
            <summary>
            Gets all tool calls from all choices.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionResult.#ctor">
            <summary>
            OpenAI generate chat completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionStreamingResult">
            <summary>
            OpenAI generate chat completion streaming result wrapper.
            This wrapper is returned immediately and contains an async enumerable stream of chunks.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionStreamingResult.Chunks">
            <summary>
            The async enumerable stream of chat completion chunks.
            Yields OpenAIChatCompletionChunk objects as they arrive from the server.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateChatCompletionStreamingResult.#ctor">
            <summary>
            OpenAI generate chat completion streaming result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest">
            <summary>
            OpenAI generate completion request.
            Note: The completions endpoint is legacy. Use chat completions for new applications.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Model">
            <summary>
            ID of the model to use (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Prompt">
            <summary>
            The prompt(s) to generate completions for (required).
            Can be a string or an array of strings.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Suffix">
            <summary>
            The suffix that comes after a completion of inserted text.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.MaxTokens">
            <summary>
            The maximum number of tokens to generate.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Temperature">
            <summary>
            Temperature for sampling (0-2).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.TopP">
            <summary>
            Nucleus sampling parameter (0-1).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.N">
            <summary>
            Number of completions to generate for each prompt.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Stream">
            <summary>
            Whether to stream partial progress.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Logprobs">
            <summary>
            Include the log probabilities on the logprobs most likely tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Echo">
            <summary>
            Echo back the prompt in addition to the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Stop">
            <summary>
            Up to 4 sequences where the API will stop generating further tokens.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.PresencePenalty">
            <summary>
            Presence penalty (-2.0 to 2.0).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.FrequencyPenalty">
            <summary>
            Frequency penalty (-2.0 to 2.0).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.BestOf">
            <summary>
            Generates best_of completions server-side and returns the best.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.LogitBias">
            <summary>
            Modify the likelihood of specified tokens appearing in the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.User">
            <summary>
            A unique identifier representing your end-user.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.Seed">
            <summary>
            Random seed for deterministic generation.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.GetPrompt">
            <summary>
            Gets the prompt as a single string.
            Throws an exception if the prompt is a list.
            </summary>
            <returns>The prompt string.</returns>
            <exception cref="T:System.InvalidOperationException">Thrown when prompt is a list instead of a single string.</exception>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.GetPrompts">
            <summary>
            Gets the prompt as an array of strings.
            If prompt is a single string, returns an array with one element.
            </summary>
            <returns>Array of prompt strings.</returns>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.SetPrompt(System.String)">
            <summary>
            Sets a single prompt string.
            </summary>
            <param name="prompt">The prompt string.</param>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.SetPrompts(System.Collections.Generic.List{System.String})">
            <summary>
            Sets multiple prompts from a List.
            </summary>
            <param name="prompts">The list of prompt strings.</param>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.SetPrompts(System.String[])">
            <summary>
            Sets multiple prompts from an array.
            </summary>
            <param name="prompts">The array of prompt strings.</param>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.IsSinglePrompt">
            <summary>
            Checks if the prompt is a single string.
            </summary>
            <returns>True if prompt is a single string, false otherwise.</returns>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionRequest.#ctor">
            <summary>
            OpenAI generate completion request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult">
            <summary>
            OpenAI generate completion result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.Id">
            <summary>
            Unique identifier for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.Object">
            <summary>
            Object type (always "text_completion").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.Created">
            <summary>
            Unix timestamp when the completion was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.Model">
            <summary>
            Model used for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.SystemFingerprint">
            <summary>
            System fingerprint.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.Choices">
            <summary>
            List of completion choices.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.Usage">
            <summary>
            Usage statistics for the completion.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.GetCreatedDateTime">
            <summary>
            Gets the created timestamp as DateTime.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.GetCompletionText">
            <summary>
            Gets the primary completion text.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.HasTruncatedChoices">
            <summary>
            Checks if any choice was truncated due to length.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionResult.#ctor">
            <summary>
            OpenAI generate completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateCompletionStreamingResult">
            <summary>
            OpenAI generate completion streaming result wrapper.
            This wrapper is returned immediately and contains an async enumerable stream of chunks.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateCompletionStreamingResult.Chunks">
            <summary>
            The async enumerable stream of completion chunks.
            Yields OpenAICompletionChunk objects as they arrive from the server.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateCompletionStreamingResult.#ctor">
            <summary>
            OpenAI generate completion streaming result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest">
            <summary>
            OpenAI generate embeddings request.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.Model">
            <summary>
            ID of the model to use (required).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.Input">
            <summary>
            Input text to embed (required).
            Can be a string or an array of strings.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.EncodingFormat">
            <summary>
            The format to return the embeddings in.
            Can be either "float" or "base64".
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.Dimensions">
            <summary>
            The number of dimensions the resulting output embeddings should have.
            Only supported in some models.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.User">
            <summary>
            A unique identifier representing your end-user.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.GetInput">
            <summary>
            Gets the input as a single string.
            Throws an exception if the input is a list.
            </summary>
            <returns>The input string.</returns>
            <exception cref="T:System.InvalidOperationException">Thrown when input is a list instead of a single string.</exception>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.GetInputs">
            <summary>
            Gets the input as an array of strings.
            If input is a single string, returns an array with one element.
            </summary>
            <returns>Array of input strings.</returns>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.SetInputs(System.String[])">
            <summary>
            Sets multiple input strings from an array.
            </summary>
            <param name="inputs">The array of input strings.</param>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.IsSingleInput">
            <summary>
            Checks if the input is a single string.
            </summary>
            <returns>True if input is a single string, false otherwise.</returns>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsRequest.#ctor">
            <summary>
            OpenAI generate embeddings request.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult">
            <summary>
            OpenAI generate embeddings result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.Object">
            <summary>
            Object type (always "list").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.Data">
            <summary>
            List of embedding objects.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.Model">
            <summary>
            Model used to generate embeddings.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.Usage">
            <summary>
            Usage statistics.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.GetEmbedding">
            <summary>
            Gets a single embedding array.
            Throws if the result contains multiple embeddings.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.GetEmbeddings">
            <summary>
            Gets all embeddings as a list of arrays.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.GetEmbeddingsArray">
            <summary>
            Gets all embeddings as a jagged array.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.IsSingleEmbedding">
            <summary>
            Checks if the result contains a single embedding.
            </summary>
            <returns>True if single embedding, false if multiple.</returns>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.IsMultiEmbeddings">
            <summary>
            Checks if the result contains multiple embeddings.
            </summary>
            <returns>True if multiple embeddings, false if single.</returns>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.GetEmbeddingCount">
            <summary>
            Gets the number of embeddings in the result.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.GetEmbeddingDimension">
            <summary>
            Gets the dimension of the embeddings.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIGenerateEmbeddingsResult.#ctor">
            <summary>
            OpenAI generate embeddings result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAILogprobs">
            <summary>
            OpenAI logprobs data.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAILogprobs.Tokens">
            <summary>
            List of token strings.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAILogprobs.TokenLogprobs">
            <summary>
            Log probabilities for each token.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAILogprobs.TopLogprobs">
            <summary>
            Top log probabilities for each token position.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAILogprobs.TextOffset">
            <summary>
            Text offset for each token.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAILogprobs.#ctor">
            <summary>
            OpenAI logprobs.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIPromptInputConverter">
            <summary>
            Converter for flexible prompt input (string or array).
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIPromptTokensDetails">
            <summary>
            OpenAI prompt tokens details.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIPromptTokensDetails.CachedTokens">
            <summary>
            Number of tokens from cached content.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIPromptTokensDetails.AudioTokens">
            <summary>
            Number of tokens from audio input.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIPromptTokensDetails.#ctor">
            <summary>
            OpenAI prompt tokens details.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIResponseFormat">
            <summary>
            OpenAI response format specification.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIResponseFormat.Type">
            <summary>
            Type of response format.
            Valid values: "text", "json_object", "json_schema"
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIResponseFormat.JsonSchema">
            <summary>
            JSON schema for structured output (when type is "json_schema").
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIResponseFormat.#ctor">
            <summary>
            OpenAI response format.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIStopInputConverter">
            <summary>
            Converter for flexible stop sequences (string or array).
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult">
            <summary>
            OpenAI streaming chat completion result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.Id">
            <summary>
            Unique identifier for the chat completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.Object">
            <summary>
            Object type (always "chat.completion.chunk").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.Created">
            <summary>
            Unix timestamp when the completion was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.Model">
            <summary>
            Model used for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.SystemFingerprint">
            <summary>
            System fingerprint.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.Choices">
            <summary>
            List of chat completion choices with deltas.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.Usage">
            <summary>
            Usage statistics (only in final chunk).
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.IsFinalChunk">
            <summary>
            Checks if this is the final chunk.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIStreamingChatCompletionResult.#ctor">
            <summary>
            OpenAI streaming chat completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIStreamingCompletionResult">
            <summary>
            OpenAI streaming completion result.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingCompletionResult.Id">
            <summary>
            Unique identifier for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingCompletionResult.Object">
            <summary>
            Object type (always "text_completion").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingCompletionResult.Created">
            <summary>
            Unix timestamp when the completion was created.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingCompletionResult.Model">
            <summary>
            Model used for the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIStreamingCompletionResult.Choices">
            <summary>
            List of completion choices.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIStreamingCompletionResult.#ctor">
            <summary>
            OpenAI streaming completion result.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAITool">
            <summary>
            OpenAI tool definition.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAITool.Type">
            <summary>
            Type of tool (currently only "function" is supported).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAITool.Function">
            <summary>
            Function definition.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAITool.#ctor">
            <summary>
            OpenAI tool.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIToolCall">
            <summary>
            OpenAI tool call.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolCall.Id">
            <summary>
            Unique identifier for the tool call.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolCall.Type">
            <summary>
            Type of tool call (currently only "function").
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolCall.Function">
            <summary>
            Function call details.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIToolCall.#ctor">
            <summary>
            OpenAI tool call.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIToolCallFunction">
            <summary>
            OpenAI tool call function details.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolCallFunction.Name">
            <summary>
            Name of the function to call.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolCallFunction.Arguments">
            <summary>
            Arguments to pass to the function as a JSON string.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIToolCallFunction.#ctor">
            <summary>
            OpenAI tool call function.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIToolFunction">
            <summary>
            OpenAI tool function definition.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolFunction.Name">
            <summary>
            Name of the function.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolFunction.Description">
            <summary>
            Description of what the function does.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolFunction.Parameters">
            <summary>
            Parameters the function accepts, described as a JSON Schema object.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIToolFunction.Strict">
            <summary>
            Whether to enable strict schema adherence.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIToolFunction.#ctor">
            <summary>
            OpenAI tool function.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAITopLogprob">
            <summary>
            OpenAI top logprob entry.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAITopLogprob.Token">
            <summary>
            The token.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAITopLogprob.Logprob">
            <summary>
            Log probability of this token.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAITopLogprob.Bytes">
            <summary>
            UTF-8 byte representation of the token.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAITopLogprob.#ctor">
            <summary>
            OpenAI top logprob.
            </summary>
        </member>
        <member name="T:SharpAI.Models.OpenAI.OpenAIUsage">
            <summary>
            OpenAI usage statistics.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIUsage.PromptTokens">
            <summary>
            Number of tokens in the prompt.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIUsage.CompletionTokens">
            <summary>
            Number of tokens in the completion.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIUsage.TotalTokens">
            <summary>
            Total number of tokens used.
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIUsage.PromptTokensDetails">
            <summary>
            Detailed breakdown of prompt tokens (for chat completions with images).
            </summary>
        </member>
        <member name="P:SharpAI.Models.OpenAI.OpenAIUsage.CompletionTokensDetails">
            <summary>
            Detailed breakdown of completion tokens.
            </summary>
        </member>
        <member name="M:SharpAI.Models.OpenAI.OpenAIUsage.#ctor">
            <summary>
            OpenAI usage.
            </summary>
        </member>
        <member name="T:SharpAI.Prompts.ChatFormat">
            <summary>
            Defines common chat/prompt formats used by different language models.
            Each format has specific tokens and structure that the model was trained to recognize.
            </summary>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.Simple">
            <summary>
            Simple role-based format using "role: content" structure.
            Compatible with many base models and general-purpose LLMs.
            </summary>
            <example>
            system: You are a helpful assistant.
            user: What is 2+2?
            assistant: 4
            user: Why?
            assistant:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.ChatML">
            <summary>
            ChatML (Chat Markup Language) format used by OpenAI models and many modern LLMs.
            Uses special tokens: &lt;|im_start|&gt;, &lt;|im_end|&gt;
            </summary>
            <example>
            &lt;|im_start|&gt;system
            You are a helpful assistant.&lt;|im_end|&gt;
            &lt;|im_start|&gt;user
            What is 2+2?&lt;|im_end|&gt;
            &lt;|im_start|&gt;assistant
            4&lt;|im_end|&gt;
            &lt;|im_start|&gt;user
            Why?&lt;|im_end|&gt;
            &lt;|im_start|&gt;assistant
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.Llama2">
            <summary>
            Llama 2 Chat format with instruction tags.
            Uses [INST], [/INST] tags and &lt;&lt;SYS&gt;&gt; for system prompts.
            </summary>
            <example>
            &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;
            You are a helpful assistant.
            &lt;&lt;/SYS&gt;&gt;
            
            What is 2+2? [/INST] 4 &lt;/s&gt;&lt;s&gt;[INST] Why? [/INST]
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.Llama3">
            <summary>
            Llama 3 format with header-based structure.
            Uses header IDs and special tokens for role demarcation.
            </summary>
            <example>
            &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
            
            You are a helpful assistant.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
            
            What is 2+2?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.Alpaca">
            <summary>
            Alpaca instruction format using ### markers.
            Commonly used by instruction-tuned models based on LLaMA.
            </summary>
            <example>
            ### System:
            You are a helpful assistant.
            
            ### Instruction:
            What is 2+2?
            
            ### Response:
            4
            
            ### Instruction:
            Why?
            
            ### Response:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.Mistral">
            <summary>
            Mistral/Mixtral instruction format.
            Uses [INST] tags similar to Llama 2 but with simpler structure.
            </summary>
            <example>
            &lt;s&gt;[INST] What is 2+2? [/INST] 4&lt;/s&gt;[INST] Why? [/INST]
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.HumanAssistant">
            <summary>
            Human/Assistant conversational format.
            Used by Anthropic's Claude and some other conversational models.
            </summary>
            <example>
            Human: What is 2+2?
            
            Assistant: 4
            
            Human: Why?
            
            Assistant:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.Zephyr">
            <summary>
            Zephyr model format with role tags.
            Uses &lt;|role|&gt; tags with &lt;/s&gt; as separators.
            </summary>
            <example>
            &lt;|system|&gt;
            You are a helpful assistant.&lt;/s&gt;
            &lt;|user|&gt;
            What is 2+2?&lt;/s&gt;
            &lt;|assistant|&gt;
            4&lt;/s&gt;
            &lt;|user|&gt;
            Why?&lt;/s&gt;
            &lt;|assistant|&gt;
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.Phi">
            <summary>
            Phi model format used by Microsoft's Phi series.
            Uses specific instruction and response markers.
            </summary>
            <example>
            Instruct: What is 2+2?
            Output: 4
            Instruct: Why?
            Output:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.ChatFormat.DeepSeek">
            <summary>
            DeepSeek model format.
            Similar to simple format but with specific markers.
            </summary>
            <example>
            User: What is 2+2?
            
            Assistant: 4
            
            User: Why?
            
            Assistant:
            </example>
        </member>
        <member name="T:SharpAI.Prompts.ChatMessage">
            <summary>
            Represents a chat message with role, content, and timestamp information.
            Used for building conversation histories in chat completion scenarios.
            </summary>
        </member>
        <member name="P:SharpAI.Prompts.ChatMessage.Role">
            <summary>
            Gets or sets the role of the message sender.
            Common values include "system", "user", and "assistant".
            </summary>
        </member>
        <member name="P:SharpAI.Prompts.ChatMessage.Content">
            <summary>
            Gets or sets the content of the message.
            Contains the actual text content of the chat message.
            </summary>
        </member>
        <member name="P:SharpAI.Prompts.ChatMessage.Timestamp">
            <summary>
            Gets or sets the timestamp when the message was created.
            Defaults to the current UTC time when the ChatMessage instance is created.
            </summary>
        </member>
        <member name="M:SharpAI.Prompts.ChatMessage.#ctor">
            <summary>
            Represents a chat message with role, content, and timestamp information.
            Used for building conversation histories in chat completion scenarios.
            </summary>
        </member>
        <member name="T:SharpAI.Prompts.PromptBuilder">
            <summary>
            Provides methods for building prompts in various chat formats used by different language models.
            </summary>
        </member>
        <member name="M:SharpAI.Prompts.PromptBuilder.Build(SharpAI.Prompts.ChatFormat,System.Collections.Generic.List{SharpAI.Prompts.ChatMessage})">
            <summary>
            Builds a formatted prompt string based on the specified chat format and messages.
            </summary>
            <param name="chatFormat">The chat format to use for formatting the messages.</param>
            <param name="messages">List of chat messages to format into a prompt.</param>
            <returns>A formatted prompt string ready for model inference.</returns>
            <exception cref="T:System.ArgumentNullException">Thrown when messages is null.</exception>
            <exception cref="T:System.ArgumentException">Thrown when messages is empty.</exception>
        </member>
        <member name="T:SharpAI.Prompts.TextGenerationFormat">
            <summary>
            Defines common text generation prompt patterns.
            </summary>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.Raw">
            <summary>
            Raw text with no formatting. The prompt is used as-is.
            </summary>
            <example>
            Write a story about a robot learning to paint.
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.Completion">
            <summary>
            Completion format where the model continues from the prompt.
            </summary>
            <example>
            Once upon a time, in a small village by the sea,
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.Instruction">
            <summary>
            Instruction following format with clear directive.
            </summary>
            <example>
            ### Instruction:
            Write a haiku about summer rain.
            
            ### Response:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.QuestionAnswer">
            <summary>
            Question-answer format for knowledge queries.
            </summary>
            <example>
            Question: What are the main causes of climate change?
            
            Answer:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.CreativeWriting">
            <summary>
            Creative writing format with genre and style hints.
            </summary>
            <example>
            Genre: Science Fiction
            Style: Descriptive, atmospheric
            Topic: First contact with aliens
            
            Story:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.CodeGeneration">
            <summary>
            Code generation format with language specification.
            </summary>
            <example>
            Language: Python
            Task: Implement a binary search algorithm
            
            ```python
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.Academic">
            <summary>
            Academic/formal writing format.
            </summary>
            <example>
            Title: The Impact of Artificial Intelligence on Healthcare
            Type: Research Summary
            
            Abstract:
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.ListGeneration">
            <summary>
            List generation format for structured output.
            </summary>
            <example>
            Create a list of 10 creative business ideas for sustainable technology:
            
            1.
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.TemplateFilling">
            <summary>
            Template filling format with placeholders.
            </summary>
            <example>
            Complete the following template:
            
            Subject: [TOPIC]
            Dear [RECIPIENT],
            
            I am writing to inform you about
            </example>
        </member>
        <member name="F:SharpAI.Prompts.TextGenerationFormat.Dialogue">
            <summary>
            Dialogue generation format for conversational content.
            </summary>
            <example>
            Characters: Alice (scientist), Bob (journalist)
            Setting: Research laboratory
            Topic: New discovery
            
            Alice:
            </example>
        </member>
        <member name="T:SharpAI.Prompts.TextPromptBuilder">
            <summary>
            Provides methods for building text generation prompts in various formats.
            </summary>
        </member>
        <member name="M:SharpAI.Prompts.TextPromptBuilder.Build(SharpAI.Prompts.TextGenerationFormat,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Builds a formatted prompt for text generation based on the specified format.
            </summary>
            <param name="format">The text generation format to use.</param>
            <param name="input">The main input text or instruction.</param>
            <param name="context">Optional context parameters specific to the format.</param>
            <returns>A formatted prompt string ready for text generation.</returns>
            <exception cref="T:System.ArgumentNullException">Thrown when input is null.</exception>
            <exception cref="T:System.ArgumentException">Thrown when input is empty or whitespace.</exception>
        </member>
        <member name="M:SharpAI.Prompts.TextPromptBuilder.BuildWithExamples(SharpAI.Prompts.TextGenerationFormat,System.String,System.Collections.Generic.List{System.ValueTuple{System.String,System.String}},System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Builds a prompt with additional context for few-shot learning.
            </summary>
            <param name="format">The text generation format to use.</param>
            <param name="input">The main input text or instruction.</param>
            <param name="examples">List of example input-output pairs for few-shot learning.</param>
            <param name="context">Optional context parameters.</param>
            <returns>A formatted prompt with examples.</returns>
        </member>
        <member name="M:SharpAI.Prompts.TextPromptBuilder.CreateContinuation(System.String)">
            <summary>
            Creates a simple continuation prompt from the given text.
            </summary>
            <param name="text">The text to continue from.</param>
            <returns>The text formatted for continuation.</returns>
        </member>
        <member name="M:SharpAI.Prompts.TextPromptBuilder.CreateInstruction(System.String,System.String)">
            <summary>
            Creates an instruction prompt with optional system context.
            </summary>
            <param name="instruction">The instruction to follow.</param>
            <param name="systemContext">Optional system context to prepend.</param>
            <returns>A formatted instruction prompt.</returns>
        </member>
        <member name="T:SharpAI.Serialization.DateTimeConverter">
            <summary>
            DateTime converter.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.DateTimeConverter.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Read.
            </summary>
            <param name="reader">Reader.</param>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">JSON serializer options.</param>
            <returns>DateTime.</returns>
        </member>
        <member name="M:SharpAI.Serialization.DateTimeConverter.Write(System.Text.Json.Utf8JsonWriter,System.DateTime,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Write.
            </summary>
            <param name="writer">Writer.</param>
            <param name="dateTimeValue">Value.</param>
            <param name="options">JSON serializer options.</param>
        </member>
        <member name="T:SharpAI.Serialization.ExceptionConverter`1">
            <summary>
            Exception converter.
            </summary>
            <typeparam name="TExceptionType">Exception type.</typeparam>
        </member>
        <member name="M:SharpAI.Serialization.ExceptionConverter`1.CanConvert(System.Type)">
            <summary>
            Can convert.
            </summary>
            <param name="typeToConvert">Type to convert.</param>
            <returns>True if convertible.</returns>
        </member>
        <member name="M:SharpAI.Serialization.ExceptionConverter`1.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Read.
            </summary>
            <param name="reader">Reader.</param>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">JSON serializer options.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Serialization.ExceptionConverter`1.Write(System.Text.Json.Utf8JsonWriter,`0,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Write.
            </summary>
            <param name="writer">Writer.</param>
            <param name="value">Value.</param>
            <param name="options">JSON serializer options.</param>
        </member>
        <member name="T:SharpAI.Serialization.IPAddressConverter">
            <summary>
            IP address converter.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.IPAddressConverter.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Read.
            </summary>
            <param name="reader">Reader.</param>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">JSON serializer options.</param>
            <returns>IPAddress.</returns>
        </member>
        <member name="M:SharpAI.Serialization.IPAddressConverter.Write(System.Text.Json.Utf8JsonWriter,System.Net.IPAddress,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Write.
            </summary>
            <param name="writer">Writer.</param>
            <param name="value">Value.</param>
            <param name="options">JSON serializer options.</param>
        </member>
        <member name="T:SharpAI.Serialization.NameValueCollectionConverter">
            <summary>
            NameValueCollection converter.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.NameValueCollectionConverter.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Read.
            </summary>
            <param name="reader">Reader.</param>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">JSON serializer options.</param>
            <returns>NameValueCollection.</returns>
        </member>
        <member name="M:SharpAI.Serialization.NameValueCollectionConverter.Write(System.Text.Json.Utf8JsonWriter,System.Collections.Specialized.NameValueCollection,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Write.
            </summary>
            <param name="writer">Writer.</param>
            <param name="value">Value.</param>
            <param name="options">JSON serializer options.</param>
        </member>
        <member name="T:SharpAI.Serialization.Serializer">
            <summary>
            Serializer.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.#ctor">
            <summary>
            Instantiate.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.InstantiateConverters">
            <summary>
            Instantiation method to support fixups for various environments, e.g. Unity.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.DeserializeJson``1(System.String)">
            <summary>
            Deserialize JSON to an instance.
            </summary>
            <typeparam name="T">Type.</typeparam>
            <param name="json">JSON string.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.DeserializeJson``1(System.Byte[])">
            <summary>
            Deserialize JSON to an instance.
            </summary>
            <typeparam name="T">Type.</typeparam>
            <param name="bytes">Bytes containing JSON.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.DeserializeJsonFromFile``1(System.String)">
            <summary>
            Deserialize JSON from a file to an instance.
            </summary>
            <typeparam name="T">Type.</typeparam>
            <param name="filename">Filename.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.SerializeJson(System.Object,System.Boolean)">
            <summary>
            Serialize object to JSON.
            </summary>
            <param name="obj">Object.</param>
            <param name="pretty">Pretty print.</param>
            <returns>JSON.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.SerializeJsonToFile(System.String,System.Object,System.Boolean)">
            <summary>
            Serialize obejct to JSON and write to a file.
            </summary>
            <param name="filename">Filename.</param>
            <param name="obj">Object.</param>
            <param name="pretty">Pretty print.</param>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.SerializeJson(System.Exception,System.Boolean)">
            <summary>
            Serialize an exception to JSON.
            </summary>
            <param name="e">Exception.</param>
            <param name="pretty">Pretty print.</param>
            <returns>JSON.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.CopyObject``1(System.Object)">
            <summary>
            Copy an object.
            </summary>
            <typeparam name="T">Type.</typeparam>
            <param name="o">Object.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.DeserializeXml``1(System.Byte[])">
            <summary>
            Deserialize XML.
            </summary>
            <typeparam name="T">Type.</typeparam>
            <param name="bytes">XML data.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.DeserializeXml``1(System.String)">
            <summary>
            Deserialize XML.
            </summary>
            <typeparam name="T">Type.</typeparam>
            <param name="xml">XML string.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Serialization.Serializer.SerializeXml(System.Object,System.Boolean)">
            <summary>
            Serialize XML.
            </summary>
            <param name="obj">Object.</param>
            <param name="pretty">Pretty print.</param>
            <returns>XML string.</returns>
        </member>
        <member name="T:SharpAI.Serialization.StrictEnumConverter`1">
            <summary>
            Strict enum converter.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.StrictEnumConverter`1.Read(System.Text.Json.Utf8JsonReader@,System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Read.
            </summary>
            <param name="reader">Reader.</param>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">JSON serializer options.</param>
            <returns>DateTime.</returns>
        </member>
        <member name="M:SharpAI.Serialization.StrictEnumConverter`1.Write(System.Text.Json.Utf8JsonWriter,`0,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Write.
            </summary>
            <param name="writer">Writer.</param>
            <param name="value">Value.</param>
            <param name="options">JSON serializer options.</param>
        </member>
        <member name="T:SharpAI.Serialization.StrictEnumConverterFactory">
            <summary>
            Strict enum converter factory.
            </summary>
        </member>
        <member name="M:SharpAI.Serialization.StrictEnumConverterFactory.CanConvert(System.Type)">
            <summary>
            Can convert.
            </summary>
            <param name="typeToConvert">Type to convert.</param>
            <returns>True if convertible.</returns>
        </member>
        <member name="M:SharpAI.Serialization.StrictEnumConverterFactory.CreateConverter(System.Type,System.Text.Json.JsonSerializerOptions)">
            <summary>
            Create converter.
            </summary>
            <param name="typeToConvert">Type to convert.</param>
            <param name="options">JSON serializer options.</param>
            <returns>JsonConverter.</returns>
        </member>
        <member name="T:SharpAI.Services.ModelEngineService">
            <summary>
            Model engine service.
            </summary>
        </member>
        <member name="M:SharpAI.Services.ModelEngineService.#ctor(SyslogLogging.LoggingModule)">
            <summary>
            Model engine service.
            </summary>
            <param name="logging">Logging.</param>
        </member>
        <member name="M:SharpAI.Services.ModelEngineService.GetByModelFile(System.String)">
            <summary>
            Get the engine for a given model file.
            </summary>
            <param name="filename">Path and filename to the model.</param>
            <returns>Instance.</returns>
        </member>
        <member name="T:SharpAI.Services.ModelFileService">
            <summary>
            Model file service.
            </summary>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.#ctor(SyslogLogging.LoggingModule,Watson.ORM.Sqlite.WatsonORM,System.String)">
            <summary>
            Model file service.
            </summary>
            <param name="logging">Logging module.</param>
            <param name="orm">ORM.</param>
            <param name="storageDirectory">Storage directory.</param>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.All">
            <summary>
            Retrieve all.
            </summary>
            <returns></returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.Enumerate(System.Nullable{System.Guid},System.Int32,System.Int32,System.Collections.Generic.Dictionary{System.String,System.String},SharpAI.Models.EnumerationOrderEnum)">
            <summary>
            Enumerate.
            </summary>
            <param name="continuationToken">Continuation token.</param>
            <param name="maxResults">Maximum number of results to retrieve.</param>
            <param name="skip">The number of records to skip.</param>
            <param name="filter">Filters to add to the request.</param>
            <param name="ordering">Ordering.</param>
            <returns>Enumeration result.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.GetByGuid(System.Guid)">
            <summary>
            Get by GUID.
            </summary>
            <param name="guid">GUID.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.GetByName(System.String)">
            <summary>
            Get by name.
            </summary>
            <param name="name">Name.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.GetFilename(System.String)">
            <summary>
            Retrieve the full path and filename of a given model by name.
            </summary>
            <param name="name">Name.</param>
            <returns>Path and filename.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.GetMany(System.Collections.Generic.List{System.Guid})">
            <summary>
            Get many.
            </summary>
            <param name="guids">GUIDs.</param>
            <returns>List.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.ExistsByGuid(System.Guid)">
            <summary>
            Exists by GUID.
            </summary>
            <param name="guid">GUID.</param>
            <returns>True if exists.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.First(ExpressionTree.Expr)">
            <summary>
            Retrieve first.
            </summary>
            <param name="expr">Expr.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.Add(SharpAI.Models.ModelFile)">
            <summary>
            Add.
            </summary>
            <param name="obj">ModelFile.</param>
            <returns>Instance.</returns>
        </member>
        <member name="M:SharpAI.Services.ModelFileService.Delete(System.Guid)">
            <summary>
            Delete.
            </summary>
            <param name="guid">GUID.</param>
        </member>
        <member name="T:SharpAI.VisionDriver">
            <summary>
            Vision driver to handle image-based prompts (base64 images) with supported engines.
            </summary>
        </member>
        <member name="M:SharpAI.VisionDriver.#ctor(SyslogLogging.LoggingModule,SharpAI.Serialization.Serializer,SharpAI.ModelDriver,System.String)">
            <summary>
            Vision driver.
            </summary>
            <param name="logging">Logging module.</param>
            <param name="serializer">Serializer.</param>
            <param name="models">Model driver.</param>
            <param name="multiModalProjectorPath">Path to a LLaVA projector GGUF.</param>
        </member>
        <member name="M:SharpAI.VisionDriver.GenerateCompletion(System.String,System.Collections.Generic.IEnumerable{System.String},System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate a vision-enabled completion by providing one or more images in base64 format
            </summary>
            <param name="model">The model identifier to use.</param>
            <param name="imagesBase64">A collection of base64-encoded images. At least one must be valid.</param>
            <param name="prompt">Optional accompanying user prompt.</param>
            <param name="maxTokens">Maximum number of tokens to generate 1024 (minimum is 100).</param>
            <param name="temperature">Sampling temperature (0.01.0).</param>
            <param name="token">Cancellation token.</param>
            <returns>The generated text response from the model.</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">Thrown if maxTokens &lt; 100 or temperature is outside [0.0,1.0].</exception>
            <exception cref="T:System.ArgumentException">Thrown if no valid images are provided.</exception>
            <exception cref="T:System.ArgumentNullException">Thrown if imagesBase64 is null.</exception>
        </member>
        <member name="M:SharpAI.VisionDriver.GenerateCompletionStream(System.String,System.Collections.Generic.IEnumerable{System.String},System.String,System.Int32,System.Single,System.Threading.CancellationToken)">
            <summary>
            Generate a streaming vision-enabled completion by providing one or more images in base64 format.
            This method provides real-time token streaming for vision model responses.
            </summary>
            <param name="model">The model identifier to use.</param>
            <param name="imagesBase64">A collection of base64-encoded images. At least one must be valid.</param>
            <param name="prompt">Optional accompanying user prompt.</param>
            <param name="maxTokens">Maximum number of tokens to generate (minimum is 100).</param>
            <param name="temperature">Sampling temperature (0.01.0).</param>
            <param name="token">Cancellation token.</param>
            <returns>An async enumerable of text chunks as they are generated.</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">Thrown if maxTokens &lt; 100 or temperature is outside [0.0,1.0].</exception>
            <exception cref="T:System.ArgumentException">Thrown if no valid images are provided.</exception>
            <exception cref="T:System.ArgumentNullException">Thrown if imagesBase64 is null.</exception>
        </member>
    </members>
</doc>
