namespace SharpAI.Engines
{
    using System;
    using System.Collections.Generic;
    using System.Linq;
    using System.Runtime.CompilerServices;
    using System.Text;
    using System.Threading;
    using System.Threading.Tasks;
    using SyslogLogging;
    using LLama;
    using LLama.Common;
    using LLama.Sampling;

    /// <summary>
    /// LlamaSharp implementation of the AI provider base class.
    /// Provides text generation, embeddings, and chat completion capabilities using the LlamaSharp library.
    /// </summary>
    public class LlamaSharpEngine : EngineBase
    {
        #region Public-Members

        /// <summary>
        /// Gets the number of dimensions in the embedding vectors generated by this engine.
        /// Returns -1 if not yet determined, 0 if embeddings are not supported.
        /// </summary>
        public override int EmbeddingDimensions
        {
            get
            {
                if (_EmbeddingDimensions == -1 && _Embedder != null)
                {
                    // Get embedding dimensions by testing with a small input
                    try
                    {
                        var testEmbeddings = _Embedder.GetEmbeddings("test").Result;
                        var testEmbedding = testEmbeddings.Single();
                        _EmbeddingDimensions = testEmbedding.Length;
                    }
                    catch
                    {
                        _EmbeddingDimensions = 0;
                    }
                }
                return _EmbeddingDimensions;
            }
        }

        /// <summary>
        /// Gets a value indicating whether this engine supports GPU acceleration.
        /// LlamaSharp supports GPU acceleration when CUDA is available.
        /// </summary>
        public override bool SupportsGpu => true;

        /// <summary>
        /// Gets a value indicating whether this engine supports generating embeddings.
        /// Support depends on whether the embedder was successfully initialized.
        /// </summary>
        public override bool SupportsEmbeddings => _Embedder != null;

        /// <summary>
        /// Gets a value indicating whether this engine supports text generation.
        /// LlamaSharp always supports text generation.
        /// </summary>
        public override bool SupportsGeneration => true;

        /// <summary>
        /// Gets whether this engine has been successfully initialized.
        /// </summary>
        public override bool IsInitialized => _IsInitialized;

        /// <summary>
        /// Gets the context window size (maximum number of tokens) for this model.
        /// </summary>
        /// <returns>The context window size in tokens, or -1 if not available.</returns>
        public override int GetContextSize()
        {
            if (!_IsInitialized || _Context == null)
                return -1;

            return (int)_Context.ContextSize;
        }

        #endregion

        #region Private-Members

        private string _Header = "[LlamaSharpEngine] ";
        private LoggingModule _Logging = null;

        private LLamaWeights _Model = null;
        private LLamaContext _Context = null;
        private LLamaEmbedder _Embedder = null;
        private InteractiveExecutor _Executor = null;
        private StatelessExecutor _StatelessExecutor = null;
        private ChatSession _ChatSession = null;
        private bool _IsInitialized = false;
        private bool _Disposed = false;
        private int _EmbeddingDimensions = -1;
        private readonly SemaphoreSlim _EmbedderSemaphore = new SemaphoreSlim(1, 1);
        private readonly SemaphoreSlim _GenerationSemaphore = new SemaphoreSlim(1, 1);

        #endregion

        #region Constructors-and-Factories

        /// <summary>
        /// Initializes a new instance of the LlamaSharpEngine class.
        /// </summary>
        /// <param name="logging">Optional logging module for capturing debug and error information. If null, a new LoggingModule will be created.</param>
        public LlamaSharpEngine(LoggingModule logging = null)
        {
            _Logging = logging ?? new LoggingModule();

            _Logging.Debug(_Header + "initialized");
        }

        #endregion

        #region Public-Methods

        /// <summary>
        /// Releases all resources used by the LlamaSharpEngine.
        /// Properly disposes of the model, context, and embedder instances.
        /// </summary>
        public override void Dispose()
        {
            if (_Disposed) return;

            try
            {
                // these do not implement IDisposable
                _ChatSession = null;
                _Executor = null;
                _StatelessExecutor = null;

                _Embedder?.Dispose();
                _Context?.Dispose();
                _Model?.Dispose();

                _EmbedderSemaphore?.Dispose();
                _GenerationSemaphore?.Dispose();
            }
            catch (Exception ex)
            {
                _Logging.Warn(_Header + "disposal exception:" + Environment.NewLine + ex.ToString());
            }
            finally
            {
                _Disposed = true;
                _IsInitialized = false;
            }
        }

        #region Initialization

        /// <summary>
        /// Initializes the LlamaSharp engine with the specified model file.
        /// Loads the model, creates context, and sets up executors for text generation and embeddings.
        /// </summary>
        /// <param name="modelPath">The file path to the GGUF model file to load.</param>
        /// <returns>A task that represents the asynchronous initialization operation.</returns>
        /// <exception cref="Exception">Thrown when the model fails to load or initialize.</exception>
        public override async Task InitializeAsync(string modelPath)
        {
            if (_IsInitialized) return;

            ModelPath = modelPath;

            await Task.Run(() =>
            {
                try
                {
                    var gpuLayers = GetOptimalGpuLayers();
                    _Logging.Debug(_Header + $"initializing LlamaSharp with {(gpuLayers > 0 ? "GPU" : "CPU")} acceleration{(gpuLayers > 0 ? $" ({gpuLayers} layers)" : "")}");

                    ModelParams parameters = new ModelParams(modelPath)
                    {
                        GpuLayerCount = gpuLayers,
                    };

                    _Model = LLamaWeights.LoadFromFile(parameters);
                    _Context = _Model.CreateContext(parameters);

                    _Executor = new InteractiveExecutor(_Context); // text generation
                    _StatelessExecutor = new StatelessExecutor(_Model, parameters); // text generation
                    _ChatSession = new ChatSession(_Executor);

                    // For embeddings, try to create a separate instance
                    try
                    {
                        ModelParams embeddingParams = new ModelParams(modelPath)
                        {
                            GpuLayerCount = gpuLayers,
                            Embeddings = true
                        };

                        LLamaWeights embeddingModel = LLamaWeights.LoadFromFile(embeddingParams);
                        _Embedder = new LLamaEmbedder(embeddingModel, embeddingParams);
                    }
                    catch (Exception ex)
                    {
                        _Logging.Warn(_Header + "failed to initialize embeddings:" + Environment.NewLine + ex.ToString());
                        _Embedder = null;
                    }

                    _IsInitialized = true;

                    _Logging.Debug(_Header + "LlamaSharp provider initialized successfully");
                }
                catch (Exception ex)
                {
                    throw new Exception("failed to initialize LlamaSharp:" + Environment.NewLine + ex.ToString());
                }
            });
        }

        /// <summary>
        /// Determines the optimal number of GPU layers to use based on available hardware.
        /// </summary>
        /// <returns>The number of GPU layers to use, or 0 if GPU acceleration is not available. Returns -1 to use all available GPU layers.</returns>
        public override int GetOptimalGpuLayers()
        {
            try
            {
                var gpuDeviceCount = LLama.Native.NativeApi.llama_max_devices();

                if (gpuDeviceCount > 0)
                {
                    _Logging.Debug(_Header + $"CUDA detected, {gpuDeviceCount} GPU device(s) available");
                    return -1; // Use all available GPU layers
                }
                else
                {
                    _Logging.Debug(_Header + "no CUDA devices detected, using CPU");
                    return 0;
                }
            }
            catch (Exception ex)
            {
                _Logging.Debug(_Header + "GPU detection exception, using CPU:" + Environment.NewLine + ex.ToString());
                return 0;
            }
        }

        #endregion

        #region Embeddings

        /// <inheritdoc />
        public override async Task<int> GetDimensionality(CancellationToken token = default)
        {
            ThrowIfNotInitialized();

            if (_Embedder == null) throw new InvalidOperationException("Embeddings are not supported. The embedder failed to initialize.");

            IReadOnlyList<float[]> embeddings = await _Embedder.GetEmbeddings("test", token).ConfigureAwait(false);
            return embeddings[0].Length;
        }

        /// <inheritdoc />
        public override async Task<float[]> GenerateEmbeddingsAsync(
            string text,
            CancellationToken token = default)
        {
            ThrowIfNotInitialized();

            if (_Embedder == null) throw new InvalidOperationException("Embeddings are not supported. The embedder failed to initialize.");

            // Handle long texts by chunking and averaging
            return await ProcessTextWithChunking(text, token).ConfigureAwait(false);
        }

        /// <inheritdoc />
        public override async Task<float[][]> GenerateEmbeddingsAsync(string[] texts, CancellationToken token = default(CancellationToken))
        {
            ThrowIfNotInitialized();
            if (_Embedder == null)
            {
                throw new InvalidOperationException("Embeddings are not supported. The embedder failed to initialize.");
            }

            float[][] embeddings = new float[texts.Length][];
            for (int i = 0; i < texts.Length; i++)
            {
                float[][] array = embeddings;
                int num = i;
                array[num] = await ProcessTextWithChunking(texts[i], token).ConfigureAwait(continueOnCapturedContext: false);
            }

            return embeddings;
        }

        #endregion

        #region Text Generation

        /// <inheritdoc />
        public override async Task<string> GenerateTextAsync(
            string prompt,
            int maxTokens = 512,
            float temperature = 0.7f,
            string[] stopSequences = null,
            CancellationToken token = default)
        {
            ThrowIfNotInitialized();

            await _GenerationSemaphore.WaitAsync(token).ConfigureAwait(false);
            try
            {
                InferenceParams inferenceParams = new InferenceParams
                {
                    MaxTokens = Math.Max(maxTokens, 100),
                    AntiPrompts = stopSequences?.ToList() ?? new List<string>(),
                    SamplingPipeline = new DefaultSamplingPipeline
                    {
                        Temperature = temperature
                    }
                };

                StringBuilder result = new StringBuilder();

                await foreach (string curr in _StatelessExecutor.InferAsync(prompt, inferenceParams, token).ConfigureAwait(false))
                {
                    result.Append(curr);
                }

                return result.ToString().Trim();
            }
            catch (Exception ex)
            {
                _Logging.Warn(_Header + "exception generating text:" + Environment.NewLine + ex.ToString());
                throw new Exception($"Failed to generate text:{Environment.NewLine}{ex.ToString()}", ex);
            }
            finally
            {
                _GenerationSemaphore.Release();
            }
        }

        /// <inheritdoc />
        public override async IAsyncEnumerable<string> GenerateTextStreamAsync(
            string prompt,
            int maxTokens = 512,
            float temperature = 0.7f,
            string[] stopSequences = null,
            [EnumeratorCancellation] CancellationToken token = default)
        {
            ThrowIfNotInitialized();

            await _GenerationSemaphore.WaitAsync(token).ConfigureAwait(false);
            try
            {
                InferenceParams inferenceParams = new InferenceParams
                {
                    MaxTokens = Math.Max(maxTokens, 100),
                    AntiPrompts = stopSequences?.ToList() ?? new List<string>(),
                    SamplingPipeline = new DefaultSamplingPipeline
                    {
                        Temperature = temperature
                    }
                };

                await foreach (var curr in _StatelessExecutor.InferAsync(prompt, inferenceParams, token).ConfigureAwait(false))
                {
                    yield return curr;
                }
            }
            finally
            {
                _GenerationSemaphore.Release();
            }
        }

        #endregion

        #region Chat

        /// <inheritdoc />
        public override async Task<string> GenerateChatCompletionAsync(
            string prompt,
            int maxTokens = 512,
            float temperature = 0.7f,
            string[] stopSequences = null,
            CancellationToken token = default)
        {
            ThrowIfNotInitialized();

            await _GenerationSemaphore.WaitAsync(token).ConfigureAwait(false);
            try
            {
                InferenceParams inferenceParams = new InferenceParams
                {
                    MaxTokens = Math.Max(maxTokens, 100),
                    AntiPrompts = stopSequences?.ToList() ?? new List<string> { "user:", "User:", "human:", "Human:" }, // Default anti-prompt for chat
                    SamplingPipeline = new DefaultSamplingPipeline
                    {
                        Temperature = temperature
                    }
                };

                StringBuilder result = new StringBuilder();

                await foreach (var curr in _StatelessExecutor!.InferAsync(prompt, inferenceParams, token).ConfigureAwait(false))
                {
                    result.Append(curr);
                }

                return result.ToString().Trim();
            }
            catch (Exception ex)
            {
                _Logging.Warn(_Header + "exception generating chat completion:" + Environment.NewLine + ex.ToString());
                throw new Exception($"Failed to generate chat completion:{Environment.NewLine}{ex.ToString()}", ex);
            }
            finally
            {
                _GenerationSemaphore.Release();
            }
        }

        /// <inheritdoc />
        public override async IAsyncEnumerable<string> GenerateChatCompletionStreamAsync(
            string prompt,
            int maxTokens = 512,
            float temperature = 0.7f,
            string[] stopSequences = null,
            [EnumeratorCancellation] CancellationToken token = default)
        {
            ThrowIfNotInitialized();

            await _GenerationSemaphore.WaitAsync(token).ConfigureAwait(false);
            try
            {
                InferenceParams inferenceParams = new InferenceParams
                {
                    MaxTokens = Math.Max(maxTokens, 100),
                    AntiPrompts = stopSequences?.ToList() ?? new List<string> { "user:", "User:", "human:", "Human:" }, // Default anti-prompt for chat
                    SamplingPipeline = new DefaultSamplingPipeline
                    {
                        Temperature = temperature
                    }
                };

                await foreach (var curr in _StatelessExecutor!.InferAsync(prompt, inferenceParams, token).ConfigureAwait(false))
                {
                    yield return curr;
                }
            }
            finally
            {
                _GenerationSemaphore.Release();
            }
        }

        #endregion

        #endregion

        #region Private-Methods

        private async Task<float[]> ProcessTextWithChunking(string text, CancellationToken token)
        {
            if (string.IsNullOrWhiteSpace(text))
                throw new ArgumentException("Text cannot be null or empty", nameof(text));

            int contextSize = (int)(_Embedder?.Context.ContextSize ?? 512);
            int tokenLimit = (int)(contextSize * 0.8);
            int charLimit = tokenLimit * 3;

            async Task<float[]> TryEmbed(string input)
            {
                await _EmbedderSemaphore.WaitAsync(token).ConfigureAwait(false);
                try
                {
                    return (await _Embedder.GetEmbeddings(input, token).ConfigureAwait(false)).Single();
                }
                finally
                {
                    _EmbedderSemaphore.Release();
                }
            }

            try
            {
                if (text.Length <= charLimit)
                {
                    return await TryEmbed(text).ConfigureAwait(false);
                }

                _Logging?.Debug(_Header + $"processing long text ({text.Length} chars) in chunks (limit: {charLimit})");
                List<string> chunks = SplitTextIntoChunks(text, charLimit);
                List<float[]> embeddings = new List<float[]>();

                foreach (var (chunk, index) in chunks.Select((c, i) => (c, i)))
                {
                    try
                    {
                        embeddings.Add(await TryEmbed(chunk).ConfigureAwait(false));
                    }
                    catch (ArgumentException ex) when (ex.Message.Contains("Embedding prompt is longer"))
                    {
                        _Logging?.Warn(_Header + $"chunk {index + 1} too long after split, retrying with smaller chunks");
                        int retryLimit = charLimit / 2;
                        var subChunks = SplitTextIntoChunks(chunk, retryLimit);
                        foreach (var subChunk in subChunks)
                        {
                            embeddings.Add(await TryEmbed(subChunk).ConfigureAwait(false));
                        }
                    }
                    catch (Exception ex)
                    {
                        _Logging?.Error(_Header + $"failed to process chunk {index + 1}: {ex.Message}");
                        throw new InvalidOperationException($"Failed to process text chunk {index + 1}/{chunks.Count}: {ex.Message}", ex);
                    }
                }

                return AverageEmbeddings(embeddings);
            }
            catch (Exception ex)
            {
                _Logging?.Error(_Header + "exception during fallback chunked embedding: " + ex.Message);
                throw;
            }
        }


        private List<string> SplitTextIntoChunks(string text, int maxCharacters)
        {
            var chunks = new List<string>();
            int currentIndex = 0;

            while (currentIndex < text.Length)
            {
                int chunkSize = Math.Min(maxCharacters, text.Length - currentIndex);
                string chunk = text.Substring(currentIndex, chunkSize);

                if (currentIndex + chunkSize < text.Length && !char.IsWhiteSpace(text[currentIndex + chunkSize]))
                {
                    int lastSpaceIndex = chunk.LastIndexOf(' ');
                    if (lastSpaceIndex > chunkSize / 2)
                    {
                        chunk = chunk.Substring(0, lastSpaceIndex);
                        chunkSize = lastSpaceIndex;
                    }
                }

                chunks.Add(chunk.Trim());
                currentIndex += chunkSize;

                while (currentIndex < text.Length && char.IsWhiteSpace(text[currentIndex]))
                {
                    currentIndex++;
                }
            }

            _Logging?.Debug(_Header + $"split text into {chunks.Count} chunks");
            return chunks;
        }

        private float[] AverageEmbeddings(List<float[]> embeddings)
        {
            if (embeddings == null || embeddings.Count == 0)
            {
                throw new ArgumentException("Embeddings list cannot be null or empty", nameof(embeddings));
            }

            if (embeddings.Count == 1)
            {
                return embeddings[0];
            }

            int dimensions = embeddings[0].Length;
            var averaged = new float[dimensions];

            foreach (var embedding in embeddings)
            {
                if (embedding.Length != dimensions)
                {
                    throw new InvalidOperationException("all embeddings must have the same dimensions");
                }

                for (int i = 0; i < dimensions; i++)
                {
                    averaged[i] += embedding[i];
                }
            }

            for (int i = 0; i < dimensions; i++)
            {
                averaged[i] /= embeddings.Count;
            }

            return averaged;
        }

        private void ThrowIfNotInitialized()
        {
            if (!_IsInitialized) throw new InvalidOperationException("Provider must be initialized before use. Call InitializeAsync() first.");
            if (_Disposed) throw new ObjectDisposedException(nameof(LlamaSharpEngine));
        }
        #endregion
    }
}